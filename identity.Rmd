# Knowing where but not what {#identity}

Imagine a parent on a museum outing saying that they are keeping track of their family members. What would they mean by that? They might mean that they are continuously aware of where each of their children are, and their spouse. They probably also mean that they are keeping track of which of them is where. Conventional object tracking tasks, however, do not assess participants' awareness of which target is where. Participants report where the targets are, but do not indicate which is which.

This chapter is about what people know about objects when they track them. We'll break the issue down into two questions. The first question is about how the position updating aspect of tracking works - does it use differences between the distractors' and targets' features to help keep track of the targets? The second question is the extent to which the features of targets are available to conscious awareness.

## The first question: Does position updating benefit from differences in object identities?

### Motion correspondence 

In industry, computer algorithms track objects for purposes such as detection of intrusions and threats to safety. They are also used in sports to analyze how the players on an opposing team move relative to each other, and they are used in animal labs to monitor the movements of study subjects. When developing their algorithms, engineers do not confine themselves to using only the locations and motions of objects - they also use the appearance of those objects, for example their shapes and colors. This facilitates matching objects across video frames (known as the correspondence problem, or in engineering as the "data association problem") [@yilmazObjectTrackingSurvey2006a]. <!--This allows successful tracking in situations where location and motion alone would result in losing a target.-->

The fact that object features would be useful for tracking does not necessarily mean, of course, that the brain actually uses them. Indeed, the division of cortical visual processing into two streams, dorsal and ventral, hints that it might not. The dorsal stream, sometimes called the "where" pathway, specializes in motion and position processing, leaving much of object recognition to the ventral stream [@goodaleSeparateVisualPathways1992]. This division raises the possibility that position updating might not involve much processing of objects' other features. <!--"Perhaps most famously, these sorts of processes seem to be localized in anatomically distinct corti- cal streams (e.g., Livingstone and Hubel 1988), with the ventral pathway corresponding to identification, and the dorsal pathway corresponding to individuation. In addition, a variety of behavioral evidence supports this distinction." The surface features of objects (e.g., their colors and shapes), while obviously critical for many visual processes including object recogni- tion, seem to be largely discounted by many other processes (for a review, see Flombaum, Scholl, and Santos, in press). For example, surface features play little or no role in determining apparent motion correspondence (Burt and Sperling 1981), identity over time in the tunnel effect (Flombaum et al., 2004; Flombaum and Scholl 2006; Michotte, Thinès, and Crabbé 1964/1991), or object-specific priming (Mitroff and Alvarez 2007)." [@schollWhatHaveWe2008]-->

Over a century ago, Gestalt psychologists such as Max Wertheimer found that apparent motion was equally strong whether the objects in successive frames had different features or identical features [@wertheimerExperimentelleStudienUber1912]. Later studies found some effect of featural similarity, but the effect was weak [@kolersFiguralChangeApparent1971; @burtTimeDistanceFeature1981]. The dominant view now is that the visual system does not use feature similarity much when computing motion correspondence to update a moving object's position. Some caution is appropriate, however, because when the successive presented frames of an object touch or overlap with each other rather than being presented in non-contiguous locations, the results can be different. The study of such displays, with a different object appearance (usually, shape) in two successive frames, is known as line motion or transformational apparent motion. These studies have found that feature similarity, especially contour continuity, but also color, can decide which tokens are matched [@faubertInfluenceTwoSpatially1995; @tseRoleParsingHighlevel1998]. Thus, feature similarity can be involved in motion processing, even though in many situations motion correspondence is instead almost completely determined by spatiotemporal luminance patterns. An important characteristic of this process that does does not seem to have been studied, however, is whether the more complex cues documented by @tseRoleParsingHighlevel1998 and others are processed in parallel. Short-range spatiotemporal luminance relationships (roughly, "motion energy") are known to be processed in parallel, by local detectors, yielding parallel visual search for a target moving in an odd direction defined by small-displacement apparent motion [@horowitzAttentionApparentMotion1994]. I am not aware of any studies that have investigated this for transformational apparent motion, in a situation where the perceived motion direction is determined by feature similarity <!--FUTURE-->. Thus, the possibility remains that feature similarity effects are driven by a capacity-one process, what I have called System 2 (\@ref(Cequals1)).

<!--Unfortunately, it is difficult to test participants on whether they know during tracking the characteristics of an object when, as soon as the question is asked, they can focus their attention on the probed object and encode its features for report. But some insight into this issue can be gained by considering the studies conducted by @makovskiFeatureBindingAttentive2009.-->

### Feature differences, but not feature conjunction differences, benefit tracking

While motion correspondence is typically driven only by spatiotemporal luminance information, another way that object featural information might benefit position tracking is via the action of feature attention. This certainly happens if the targets differ simply in color from the distractors. It is well-established that attention can select stimulus representations by their color. One can, for example, enhance the selection of all red objects in the visual field. @makovskiFeatureBindingAttentive2009 confirmed that this process can benefit MOT. They used eight moving objects, four of which were targets. MOT performance was better than when the eight objects were different in color than when they were identical. This was also true when the objects were all different in shape.

Apart from the usefulness of attending to an individual feature when the targets differ in that feature from the distractors, do feature differences benefit tracking? A large body of evidence has supported Treisman's theory that feature pairing information, in contrast to individual features, cannot efficiently guide attention to targets [ @treismanFeatureIntegrationTheory1980; @wolfeGuidedSearchUpdated2021]. It seems that the splitting of attention among multiple locations that occurs in MOT is similar to how attention is diffused over multiple stimuli in visual search and other paradigms, because @makovskiFeatureBindingAttentive2009 found that targets having unique feature pairings do not benefit tracking performance. In their "feature conjunction" condition, each object had a unique pair of features, while it shared the individual features with at least one other object. <!--create illustration maybe from diagrammeR--> Performance was no better in this condition than if the objects were all identical. It is this pairing situation that prevents featural attention from contributing, and the results suggest that the tracking process itself does not use featural differents.

<!--Treisman, as well as many subsequent researchers, went further than this and theorized that feature pairing information only becomes available if attention focuses on -->

## The second question: Are we aware of the identities and features of objects we are tracking?

## Feature updating

A common view among lay people may be that we are simultaneously aware of the identities of all the objects in the central portion of our visual field, so unless an object actually disappears, hides behind something or someone, or moves to the edge of our visual field, we should always know where everything in the scene is, and we should readily detect any changes to these objects. Change blindness demonstrations expose this as untrue.

Change blindness experiments typically use stationary, albeit changing, objects, and suggest that although people cannot simultaneously monitor a large number of objects for change, they are able to monitor several, perhaps four or five [@rensinkVisualSearchChange2000]. They appear to do this by loading selected objects into working memory and then, in the second frame of a change blindness display, checking whether any are different than what is held in memory.<!--People certainly can store several objects and rapidly compare these stored representations to the visual scene-->. This ability to load into memory the features of objects for storage and subsequently comparing them to a new display with the objects in the same location may have different demands than continuously updating awareness of the changing features of objects. For one thing, it appears that hundreds of milliseconds are needed to encode several objects into memory [@vogelTimeCourseConsolidation2006; @ngiamVisualWorkingMemory2019].<!-- In addition, it appears that when objects are in motion, updating of their features is particularly poor, as we will see-->

With a visual transient to call attention to the site of a change, the brain is overwhelmed by the task of updating the features of the objects in a typical scene. This is even more true of scenes with moving objects than stationary, because motion means continuous transients, which masks the transient caused by a featural change. An example of the failure to detect changes to even a limited number of objects was provided by @saikiMultipleobjectPermanenceTracking2002, who had participants view a circular array of colored discs that rotated about the center of the screen. Occasionally discs swapped color when they briefly went behind occluders, and the participants' task was to detect these color switches. Performance decreased dramatically with disc speed and number of discs, even though the motion was completely predictable, and @saikiMultipleobjectPermanenceTracking2002 concluded that "even completely predictable motion severely reduces our capacity of object representations, from four to only one or two." Because we now understand that simple MOT does not work well across occluders, however, that interpretation of the study is limited by the absence of an MOT-type control. This was taken further by @saikiBlindnessSimultaneousChange2012 with out occluders, using a field of 200 moving dots. In one condition, half were green and half were red and the task was to detect a sudden change in color of all the dots. Even when all 200 dots simultaneously switched color between red and green, performance in detecting the switch was very poor. Why had such a dramatic change blindness phenomenon never been noticed before? The phenomenon only occurred when the relative proportion of the two colors was approximately the same before and after the switch, indicating that what is sometimes called "summary statistics" for the overall display are updated readily, but individual pairings of dots with colors are not. This phenomenon was later made into an even more dramatic demonstration by @suchowMotionSilencesAwareness2011. A full explanation of the phenomenon continues to be debated, but I think it makes it clear that non-position feature updating is less likely to occur with a moving object than with stationary objects. For a stationary object, a feature change will typically stimulate motion / transient detectors, drawing attention to the change and triggering an update. Not so with moving objects, as the motion detectors are continually stimulated, so a feature change does not yield an attention-drawing transient.

Even when objects are stationary, featural updating can be slow. @howardTrackingChangingFeatures2008 investigated feature updating by having moving Gabor targets continually change in their orientation or spatial frequency. After a random interval of this continuous change, all the objects disappeared and the location of one was cued - the task was to report its last feature value. Participants tended to report a feature value that the object had earlier in time than the last frame, as one would expect due to either a feature integration time or intermittent updating. What was more interesting was that this lag increased with the number of objects monitored. In the spatial frequency conditions, the average lag was 140 ms when monitoring one Gabor, 210 ms for tracking two Gabors, and 250 ms for tracking four Gabors. The lags increased also for monitoring orientation and for monitoring location, although not nearly as much: 1 orientation = no measurable lag, 2 orientations = 10 ms, and four orientations = 40 ms, and for position 40 ms, 50 ms, and 90 ms. It's possible that a position cue contributed to the orientation reporting. If the objects were put in motion, it is possible that performance and the effect of target load would be even worse, but to my knowledge this has never been investigated<!--FUTURE-->. The results of some other behavioral paradigms also point to sluggish feature updating
[@holcombeTemporalBindingFavours2009; @callahan-flintoftDelaySamplingInformation2020].

Such results suggest a limited-capacity system is required for updating some features, which is not very surprising, whereas position updating seems less capacity-constrained. One might hope that even with a limited-capacity system of feature updating, however, that simple maintenance of the features of objects as they move could easily be done. Instead, maintenance of tracked targets' identities can be very poor, as we will see in the next section. This is one of the most striking findings in the literature.

## Maintenance of target features and identities

<!--FUTURE With the motion silencing illusion, it goes not occur for an object that one is tracking. I don't think any papers exist that make this point, or tell us whether the reason is because you're parsing away the motion from the change signal, or whether you actually have to compare feature values. Related phenomena are temporal camouflage and Saiki's original moving change blindness with occlusion findings . It also probably explains why infants don't process object features, because there is no cue to get them to update the object features.  Also the Wikipedia entry needs lots of work and should cite Saiki & Holcombe -->

<!-- A powerful demonstration that does not fully show lack of awareness of identities, but certainly shows how motion can obscure change in identities, is the motion silencing illusion [@suchowMotionSilencesAwareness2011]. If you have not seen it, I high recommend you [have a look](https://michaelbach.de/ot/mot-silencing/). The lack of awareness of the color changes succeeds partly because the color changes do not change the overall proportion of each color at a global level - this phenomenon was documented by @saikiBlindnessSimultaneousChange2012 in work prior to the discovery of the illusion. Another ingredient is the density of the display, which discourages ACTUALLY YOU CAN NOTICE THE CHANGES IF YOU TRACK, SO HOW IS THIS RELEVANT? -->

According to Zenon Pylyshyn's FINST (Fingers of Instantiation) theory of tracking, a small set of discrete pointers are allocated to tracked targets. Pylyshyn's idea was that a pointer allows other mental processes to individuate and link up with an object representation, and the pointer's continued assignment to a target facilitates representing the corresponding object as the same persisting individual [@pylyshynRoleLocationIndexes1989]. This implies that when tracking multiple targets, people should know which target is which. However, when Pylyshyn assessed this, the results turned out differently than he expected. The first of two papers he wrote on the topic was entitled "Some puzzling findings in multiple object tracking: I. Tracking without keeping track of object identities". Targets were assigned identities either by giving them names or by giving them distinct and recognizable starting positions: the four corners of the screen [@pylyshynPuzzlingFindingsMultiple2004]. At the end of a trial, participants were given the standard task of indicating which objects were targets, but also were asked about the identity of the target - which one it was. Accuracy at identifying the targets was very low, even when accuracy reporting their positions was high. This result was robust to a number of task variations. <!-- target-target confusions were found to mostly explain this --> <!--oksamaPositionTrackingIdentity2016 found for set size=5 much more errors in MIT, not so much for set sizes 2,3,4 but could be a floor effect--> However, the task was always to report all the locations first and report the identities second, raising the possibility that the need to remember the identities for longer could have contributed to the poorer associated performance.

More evidence for a disconnect between knowledge of what one is tracking and success at the basic MOT task was found by @horowitzTrackingUniqueObjects2007, who had participants track targets with unique appearances - the stimuli were cartoon animals in one set of experiments. At the end of each trial, the targets moved behind occluders so that their identities were no longer visible. Participants were asked where a particular target (say, the cartoon rabbit) had gone - that is, which occluder it was hiding behind. This type of task had been dubbed "multiple identity tracking" by @oksamaMultipleObjectTracking2004. Performance was better than chance, but was worse than the standard MOT task of reporting target locations irrespective of which target a location belonged to. This basic finding was replicated in four additional experiments. The effective number of objects tracked, as reflected in a standard MOT question, was around three or four, but for responses about the final location of a particular animal, capacity was estimated as closer to two objects. <!--REVIEWER: Regarding the testing of tracking capacity in MIT, one possibility is to use the change blindness paradigm. To my knowledge, there are at least two studies that have applied it to estimate the capacity in MIT (Oksama & Hyönä, 2008; Wu & Wolfe, 2016). Interestingly, they both point to a capacity of about two items.--> So, the evidence seems robust that knowledge of which target is which can easily fall away, in contrast to Pylyshyn's original view that this information was part and parcel of the tracking mechanism. These results of MIT experiments suggest that our ability to update the location of one of multiple objects of interest is much better than our ability to maintain knowledge of what that object is.

One counterpoint is that a recent study by @wuComparingEyeMovements2018 found only a fairly small performance deficit for identity reporting relative to position reporting. Using MOT and MIT tasks carefully designed to be comparable, they had participants track 3, 4, or 5 targets, and found 96%, 89%, and 86% accuracy for the MOT task, against 93%, 85%, and 79% accuracy for the MIT task. While the high performance for the 3-target condition could be a ceiling effect, that is probably not the case for the 4- and 5-target conditions. One difference with previous work is that at the beginning of a trial, all the stimuli (cartoon animals) were stationary and participants had unlimited time to memorize the targets’ locations. When a participant was ready, they would press a key and the animals transformed into identical gray circles and began moving. At the end of the trial, one of the circles was probed and the participant either had to indicate whether it was a target, or indicate
whether it had originally been a particular animal. One possible explanation of the discrepancy with previous findings is that while identities are not native to tracking's pointers in the way Pylyshyn thought, with adequate time for memorization the associations can be made and maintained.

<!-- This harkens back to Pylyshyn's idea that tracking is mediated by pointers that in and of themselves, only point to locations and don't contain other featural information. Pylyshyn thought that these pointers, being unique and distinct, do provide us with knowledge of which target location at the end of a trial corresponded to a particular target at the beginning of a trial. However, his own experiments ruled against that - the tracking process seems to deploy something to the moving targets that carries absolutely no information about those targets other than their positions.-->

### Beaten by a bird brain

<!--For instance, @schollRelationshipPropertyencodingObjectbased2001 found that when items stopped moving, observers were able to accurately report the previous direction and speed of targets but not of nontargets. However, when the shape or color of the items was masked, observers were unable to accurately report the premask features of either targets or nontargets.-->

@pailianAgeSpeciesComparisons2020 investigated identity maintenance during tracking in a slightly different way. Identical objects were assigned unique identities, in a format like a hustler's shell game. The engaging nature of the shell game format made it suitable for testing children and an African grey parrot as well as human adults. This led to a few surprises.

As stimuli, @pailianAgeSpeciesComparisons2020 used colored wool; real balls of wool, actually, not mere pictures on a screen. Between one and four of the balls were shown to a participant by the experimenter. The experimenter then covered the balls with inverted opaque plastic cups, and began to move them, swapping the positions of first one pair, then another. After a variable number of pairs were swapped, the experimenter produced a probe ball of one of the target colors, and the participant's task was to point to (or peck on, in the case of the parrot), the cup containing the probed color.

```{r Pepperberg, echo=FALSE, out.width="40%", fig.cap="An African grey parrot participates in the shell game used by Pailian et al. (2020). CC-BY Hrag Pailian."}
knitr::include_graphics("imagesForRmd/ParrotGriffinPepperbergShellGame.png")
```

At any one time, only two objects were in motion. The participants were responsible, however, for knowing the final location of all the colors - in other words, there were no distractors. Before seeing the results, I would have predicted that people would be able to perform this task with high accuracy, especially given that not only were only two objects in motion at any one time, the experimenter paused for a full second between swaps, which ought to give people sufficient time to update their memory of the locations of those two colors. When only two balls were used, accuracy was in fact high: over 95%, even for four swaps, which was the highest number tested. This was true for the human adults, the parrot, and the (6- to 8-year-old) children alike. 

In the three-ball condition, the adults did fine when there were only a few swaps, but their performance fell substantially as the number of swaps increased, to about 80% correct for four swaps. For some reason, participants did not reliably update the colors for four swaps. The effect of number of swaps was more dramatic for the children. They performed near ceiling for the zero-swap (no movement) condition, but accuracy fell to close to 80% in the one-swap condition, and to around 70% for two and three swaps. 

Remarkably, the parrot actually outperformed not only the children, but also the human adults. It seems that this was not due to more practice - 
 the parrot had not been trained extensively on the task; the authors state that the parrot learned it primarily by simply viewing the experimenter and a confederate perform three example trials (the parrot was experienced with a simpler version of the task involving only one object presented under one of the three cups). To me, the fact that an African gray parrot had the ability to remember and update small sets of moving hidden objects to a level of accuracy similar to humans, despite having a brain less than one-fiftieth the size of ours, helps to highlight that the task is quite simple. <!--Of course, this was an above-average parrot (selection bias surely was part of the reason it has been studied extensively), but still. Large parts of the parrot brain evolved after they split from our lineage [@iwaniukInterspecificAllometryBrain2005], so the fact that it can do this task, like us (or even better than us), appears to be an example of convergent evolution.-->

What needs to be explained is why the adult humans (in this case, Harvard undergraduates, who almost surely had above-average intelligence and motivation) displayed levels of accuracy that were not very high for the conditions that involved more than a few swaps. Remember that in these experiments, only two balls were moved at a time, and there was a one-second pause between swaps. Prior to the publication of this study, I had assumed that the reason for poor performance in multiple identity tracking was the difficulty of updating the identity of three or four targets simultaneously while they moved. I would have predicted that changing positions exclusively by swapping the positions of two objects, and providing a one-second pause between swaps, would keep performance very high. These results of @pailianAgeSpeciesComparisons2020 suggest that updating the memory of object locations is quite demanding. Thus, not only does identity updating not happen automatically as a result of object tracking, but also it may rely on a very sluggish memory updating system.

Another reason to be surprised is the venerable standing of the "object files" phenomenon in the literature. The theory is that all the features of an object are associated with a representation in memory, the object file, that is maintained even as an object moves [@kahnemanReviewingObjectFiles1992]. In associated experiments pioneered by @kahnemanReviewingObjectFiles1992, a trial begins with a preview display with two rectangles. Each rectangle contains a feature - in most experiments, a letter. The featural information then disappears, and the rectangles move to a new location. The observer's representation of the display is then probed by presenting a letter once again in one of the rectangles, or elsewhere, and asking participants to identify it. If the letter is the same as the one presented in that rectangle at the beginning of the display, observers are faster to respond than if it had appeared in another rectangle in the beginning of the display, indicating that that aspect of the rectangle's initial properties was maintained, with its location updated. One difficulty with interpreting this response time priming phenomenon is that, because responses must be averaged over many trials to reveal it, we do not know on what proportion of trials it is effective. Thus it is hard to know whether it is consistent with the behavioral findings mentioned above that show successful updating on only a minority of trials.

I think many researchers may have made the same mistake as I of assuming that object files were updated and effective on every trial. There is also the question of the capacity of the object-file system: whether several object files could easily be maintained and updated. @kahnemanReviewingObjectFiles1992 found that the amount of priming was greatly diminished when four letters were initially presented in different rectangles, suggesting that fewer objects than that had letter information maintained and updated. They concluded that there may be a severe capacity limit on object files or object file updating. <!--FUTURE: Use Noles & Scholl paradigm, much biger effect-->  Nevertheless, the evidence from the studies in this chapter overall suggests that identity updating is very poor in a range of circumstances.

### Some dissociations between identity and location processing reflect poor visibility in the periphery

To explain why participants don't update the identities of the moving objects that they are tracking nearly as well as they update their positions, the Finnish researchers Lauri Oksama and Jukka Hyönä suggested that identities are updated by a serial one-by-one process, while positions are updated in parallel. Oksama & Hyönä were motivated by a line of evidence that I have mentioned yet, from eye tracking. @oksamaPositionTrackingIdentity2016 contrasted eye movements during MOT with eye movements during MIT, using line drawings of objects as the stimuli. During MIT, participants frequently looked directly at targets, for more than 50% of the trial duration, and frequently moved their eyes from one target to another. In contrast, during MOT, the participants moved their eyes infrequently, and their gaze wasn't usually at any of the moving objects, rather they were more often looking somewhere close to the center of the screen. @oksamaPositionTrackingIdentity2016 took these results to mean that the targets' identity-location bindings that must be updated during MIT are updated by a serial one-by-one process, whereas target positions during MOT are updated by a parallel process; for a review, see @hyonaEyeBehaviorMultiple2019.

A problem for interpreting the @oksamaPositionTrackingIdentity2016 results is that participants may have had to update target identity information one-by-one purely due to limitations on human peripheral vision. That is, the targets (line drawings of different objects) likely were difficult to identify when in the periphery. Thus, participants may have had to move their eyes to each object to refresh their representation of which was which. Indeed, in a subsequent study, @liModelMultipleIdentity2019 tested discriminability of the objects in the periphery and found that accuracy was poor. <!-- The conclusion by @oksamaPositionTrackingIdentity2016 that identities are updated by a serial one-by-one process that determines the movement of the eyes may well be correct.--> When colored discs were used as stimuli instead of line drawings, accuracy was higher in the periphery and participants did not move their eyes as often to individual targets, although they still did so. This reduction in saccades to individual targets suggests at least some degree of parallel processing, leaving the amount of serial processing for simple colors, if any, in doubt.

<!--@oksamaPositionTrackingIdentity2016 did not assess how far in the periphery the target objects were identifiable - the only study I know of that did this is a later paper by the Finnish team, specifically @liModelMultipleIdentity2019. They tested discriminability of line drawings from the same database as those used by @oksamaPositionTrackingIdentity2016, while also testing the discriminability of faces and colored discs. More specifically, they first presented a sample stimulus at fixation (either a face, a line drawing, or a colored disc, depending upon the block of trials) and subsequently in the periphery they presented either the same stimulus or another stimulus from the same category. The task of participants was to judge whether this peripherally-presented stimulus was the same or different. For the faces and the line drawings, percent correct decreased a substantial amount with distance from the fovea. However, for the color task, percent correct was the same at all three eccentricities tested (2.5, 5, and 7.5 deg).    Forget a bit about what it looks like. don't have to classify among all the options-->

Many findings of differences between MIT and MOT performance may be explained by poor recognition of the targets in the periphery. Because most studies of MIT do not include an assessment of how recognizable their stimuli are in the periphery (@liModelMultipleIdentity2019 is the only study I know of that did this), it is hard to say how much of the difference between MIT and MOT can be attributed to this. I am not sure how one should equate object localization with object identifiability. One could blur the objects to impair localization but it is not clear what degree of spatial uncertainty is comparable to a particular level of object identifiability; this is the old apples-and-oranges problem.

One dissociation between identity and location tracking performance seems to remain valid regardless of the difficulty of perceiving object identities in the periphery. This is the original finding by @pylyshynRoleLocationIndexes1989, replicated by @cohenWhatwhereTradeoffMultipleidentity2011, that if targets are actually identical but are assigned different nominal identities, participants are very poor at knowing which is which at the end of the trial. Because in this paradigm, there is no visible identity information and participants knew this, poor vision in the periphery is not an issue.

### Evidence from two techniques suggests parallel updating of identities

<!-- To some extent, however, we're comparing apples and oranges. This study should be replicated with another sample of participants, however, and using a method that directly compares MIT (knowledge of target identities) to MOT (knowledge of targets' locations) by including distractors, not just targets. Without that, there is some concern that the presence of the experimenter was distracting or that some other idiosyncrasy of the paradigm led to poor identity performance.--> 

<!--These claims of a process that is serial and one-by-one are not based on rigorous tests of serial processing like those progressively elaborated by Jim Townsend and colleagues since the 1980s [@algomFeaturesResponseTimes2015; @townsendSerialVsParallel1990]. Rather, it is based on more indirect evidence such as the visual search results that originally led Anne Treisman to propose a serial feature binding process.-->

@howeIdentityLocationBindingProblem2015a used two techniques to investigate the possibility that serial processes are involved in multiple identity tracking. First, Howe et al. applied a simultaneous-sequential presentation technique that when applied to MOT had previously yielded evidence for no serial processing [@howeDistinguishingParallelSerial2010]. In the technique, which was originally developed by @shiffrinVisualProcessingCapacity1972 to investigate briefly-presented stationary stimuli, the stimuli are presented either all at once (simultaneously) or in succession (sequentially). In the successive condition, half the stimuli presented in the first interval of a trial, and the other half in the second interval. If a serial process is required to process each stimulus, the prediction is that performance should be better in the sequential condition, as the presentation duration of each stimulus is equated for the simultaneous and successive conditions, but in the simultaneous condition a one-by-one process wouldn't have enough time to get through all the stimuli. The technique has been applied extensively to the detection of a particular alphanumeric character among other alphanumeric characters, and researchers have found that processing in the simultaneous condition is equal to or better than the sequential condition, suggesting that at least four alphanumeric characters can be recognized in parallel [@shiffrinVisualProcessingCapacity1972; @hungSimultaneousBetterSequential1995].

For the MIT simultaneous-sequential paradigm devised by @howeIdentityLocationBindingProblem2015a, four targets of different colors moved among four distractors. Each of the four distractors was the same color as one of the targets, so that the targets overall could not be distinguished from the distractors by color (which is important, as explained above in [Feature differences, but not feature conjunction differences, benefit tracking]). In the simultaneous condition, all the objects moved for 500 ms and then paused for 500 ms, a cycle that repeated throughout the trial. In the sequential condition, half the targets moved for 500 ms while the other half were stationary, and subsequently the other half of targets moved for 500 ms while the others remained stationary, throughout the length of the trial. Performance was similar in the simultaneous and sequential conditions, supporting the conclusion that there was no serial process required for the task [@howeIdentityLocationBindingProblem2015a]. This conclusion is limited, however, by an assumption that any serial process could respond efficiently to the movement cessation of half the targets by shifting its resources to the moving targets, while not causing any forgetting of the locations and identities of the temporarily-stationary targets. To support this assumption, @howeIdentityLocationBindingProblem2015a pointed out that @hogendoornTimeCourseAttentive2007 had shown that attention could move at much faster rates than 500 ms per shift. However, the @hogendoornTimeCourseAttentive2007 studies did not assess the attention shifting time between unrelated targets, rather their shifts were for attention stepping along with a single target disc as it moved about a circular array. Thus, it is unclear how much the results of @howeIdentityLocationBindingProblem2015a undermine the serial, one-by-one identity updating idea embedded in the theories of Oksama & Hyönä and @lovettSelectionEnablesEnhancement2019.

@howeIdentityLocationBindingProblem2015a further investigated serial versus parallel processing in MIT by using another technique: the systems factorial technology of Jim Townsend and colleagues [@townsendSerialVsParallel1990]. Two targets were designated for tracking and presented in the same hemifield, to avoid independence by virtue of the hemispheres' independence [@alvarezIndependentResourcesAttentional2005]. The participants were told to monitor both targets as they moved and that if either of the targets darkened, to press the response button as quickly as possible, after which all the disks stopped moving and the participant was asked to identify the location of a particular target, for example the green one (the objects were identical during the movement phase of the trial but initially each was shown in a particular color). To ensure that participants performed the identity tracking task as well, only trials in which the participant reported the target identity correctly were included in the subsequent analysis. Detection of the darkening events was very accurate (95% correct). On different trials, either both targets darkened, one of them darkened, or neither of them darkened, and each could darken either by a small amount or by a large amount. <!--Because it's a response time analysis, it's measuring parallel or serial processing, which may be different somehow from possibly serial processing for individuating a target or solving the correspondence problem for it. In that case, the actual processing time is time-limited by the stimulus, whereas you can let a slow parallel accumulator eventually do its thing when the stimulus information isn't time-curtailed. In other words, attention might focus on one because it absolutely has to get done, but that would mean switching its strategy adaptively based on difficulty--> The pattern of the distributions of response time for the various conditions ruled out serial processing (if one accepts certain assumptions)  and implicated limited-capacity parallel processing. This suggests that participants can process luminance changes of two moving targets in parallel while also maintaining knowledge of the identity of the moving targets. One reservation, however, is that it is unclear how often the participants needed to update the target locations and refresh their identities, because the rate at which they needed to be sampled to solve the correspondence problem is unclear for the particular trajectories used (this issue is explained in @holcombeObjectSeparationTime2022). <!--The fact that identity tracking accuracy was over 95% suggests that the task was easy, but still--> It also would be good to see these techniques applied to targets defined only by distinct feature conjunctions, with no differences in features between the targets and the distractors. This would prevent any contribution of feature attention, and with processing of feature pairs likely to be more limited-capacity than that of identifying individual features, the results might provide less evidence for parallel processing.

<!--PARTIALLY IN RESPONSE TO THIS, MOMIT 2.0 UPDATED -->

## Eye movements can add a serial component to tracking

Partially in response to the evidence of @howeIdentityLocationBindingProblem2015a against serial processing in tracking, Oksama and Hyona, and a colleague Jie Li, revised their Model of Multiple Identity Tracking (MOMIT) to add more parallel processing. MOMIT 2.0 claims that the "outputs of parallel processing are not non-indexed locations but proto-objects that contain both location and basic featural information, which can be sufficient for tracking in case no detailed information is required" [@liModelMultipleIdentity2019]. This is a reasonable response to the evidence, even if it unfortunately means the theory doesn't make as strong predictions, as the role of serial processing is now more vague. In this model, serial processing is tied to eye movements and is used to acquire detailed visual information for refreshing working memory representations. @liModelMultipleIdentity2019 wrote that this "prevents the resolution of the active representations from declining. This is vital for tracking targets that require high-resolution information to be identified and kept distinguishable from other targets." The theory seems to be mute on whether serial processing would be involved if both fixation were enforced and the stimuli were easily identifiable in the periphery.

Here I'd like to step back and consider the role of eye movements in more everyday behavior. People move their eyes on average three times a second, in part because our retina has a specialized part (the fovea) that it is usually adaptive to direct at whatever object we are most interested in. In natural tasks, rarely is it the case that all  visual signals of interest are clustered together enough that they can be processed adequately without moving the fovea among them. Moreover, animals like ourselves have strong drives for exploration and vigilance regarding visual scenes, because we evolved in changing environments.

Eye movements usually contribute a serial, one-by-one component to processing, because as @liModelMultipleIdentity2019 highlighted, high-resolution information comes from only a single region on the screen - the region falling on the fovea. Near-continual scanning of the visual scene is an ingrained habit, and is also necessary for many artificial tasks, like reading. Not only are saccades frequent during reading, one influential theory of reading proposed that an internal rhythm drives saccades from one word to the next rather than them being triggered by the completion of a process such as word recognition [@engbertDynamicalModelSaccade2002]. Perhaps, then, one should expect frequent eye movements to occur and contribute a serial component of processing to a range of tasks even when eye movements are not necessary. People are cognitively "lazy" in that they seem to structure eye movements and other actions in tasks so as to minimize short term memory requirements [@hayhoeTaskConstraintsVisual1998]. Thus, even if saccading to different targets were inefficient because people could keep information in memory, and update information in the periphery, people may move their eyes anyway.

To advance the serial/parallel processing debate, we should recognize that the frequent eye movements associated with natural behavior should be expected in tracking, as for any task involving multiple relevant stimuli, and this can contribute a serial processing component, even if people can perform the same task in a much more parallel fashion when eye movements are constrained. The most interesting evidence for serial processing, then, may be that found when eye movements are prohibited. The steep increase with load in apparent sampling frequency discovered by @holcombeSplittingAttentionReduces2013 constitutes some evidence for that.

<!--The Finnish team of Lauri Oksama and Jukka Hyönä may have done the most studies of the task they dubbed multiple identity tracking (MIT) in 2004 [@oksamaMultipleObjectTracking2004], introduced above in the version used by @horowitzTrackingUniqueObjects2007. Based on results includning those I have reviewed here, since 2008 Oksama and Hyönä have been advocating a model that tracks the locations of moving objects with a parallel process, but updates other featural information with a serial, one-by-one process [@liModelMultipleIdentity2019]. -->

<!--They say Holcombe & Chen 2013 found big evidence for serial because "when objects move along the same trajectories (e.g., Holcombe & Chen, 2013) and/or are close to each other, high resolution information is required for discriminating the objects. Thus, tracking becomes more serial" but they don't seem to say anything about how different trajectories benefits performance-->


<!--Preventing the use of feature attention
The effectiveness of featural attention raises an important issue for the interpretation of MIT studies. All the MIT studies I reviewed above did not have targets and distractors distinguished only by their feature conjunctions; rather, they typically used unique shapes such as cartoon animals or line drawings. This raises the possibility that the targets sometimes shared some feature or features with each other more than with the distractors. In other words, feature attention could help separate the targets from the distractors. This could be avoided by using pairings of features, as in the feature conjunction condition used by @makovskiFeatureBindingAttentive2009 for MOT. Another approach was used by Piers Howe and colleagues who paired each target, which had a distinct appearance, with a distractor that had an identical appearance.-->

To summarise this chapter, there is plenty of evidence that both use of object identities in tracking and the updating of target identities for awareness is quite poor. This fits with a much broader set of findings over the last thirty years, that the mind maintains fewer explicit visual representations than we intuitively believe. The first findings of change blindness, which were in the context of failures to detect changes that occurred during eye movements [@mcconkieRoleControlEye1979], led @oreganSolvingRealMysteries1992 to suggest that "the world is an outside memory" and later to discover change blindness [@rensinkSeeNotSee1997]. The idea was that the impression that one has a rich representation of all the objects in the visual field is an illusion, and instead that one has only a more limited knowledge, but that this is quickly supplemented by attentional processing when one becomes interested in a particular location or object. It appears that O'Regan's big idea goes further than he anticipated. While O'Regan suggested that only when objects were attended would they be fully processed, he did not suggest that one might be able to track the changing locations of multiple targets without becoming aware of what they are. Such lack of awareness has been documented not only in the context of tracking multiple objects as reviewed above, but even in a change blindness task that involved eyetracking of just a single target as part of an ongoing, more complex task [@trieschWhatYouSee2003].


<!--@hudsonHemifieldEffectsMultiple2012 multiple identity tracking , found a hemifield advantage: " Contrary to expectations, a bilateral advantage was still observed, though it was not as strong as when observers were not required to remember the identities of the targets. This finding is inconsistent with the only model of multiple identity tracking (Oksama & Hyönä, 2008, Cognitive Psychology, 56, 237-283), so we present an alternative account."-->
<!-- E4 standard MOT design. Calculate hemifield independence = 
A weird thing is that there were no distractors in the same quadrant ,but E3 used the standard design and found the same result.
E1: colors of the targets continuously visible. Found substantial bilateral advantage.
E2: colors of targets only presented at beginning. Substantial bilateral advantage again.
-->

<!--Tracking is important for representing objects still in view @tsubomiNeuralLimitsRepresenting2013-->


<!--
A second factor is that even when objects remain close enough to central vision to resolve their identities, a limited resource is required to bind their features together, including binding individual features with their location... 

A third factor is, of course, the limitations that result in imperfect performance even on tracking identical objects, reviewed in the previous sections..

You can track four objects without being able to identify stuff on the four objects. This could either be explained by attention needing to switch between locations, or alternatively that you just need less resource to track than to identify.

Because binding seems to require sustained attention, it’s unlikely it’s bound when serial attention isn’t there. But you might have a memory index of what is there - I need help on this from VWM people!-->


<!-- ## Configurations and 

Most models of tracking have assumed that people track only the targets, and track them without reference to the positions of any other objects. We know from other areas of research, however, that people rapidly extract that spatial layout or configuration of the objects in a scene. Some research has indicated that configural information is also used for tracking.


Because binding seems to require sustained attention, it’s unlikely it’s bound when serial attention isn’t there. But you might have a memory index of what is there - I need help on this from VWM people!

Linares et al. ()


Let's return to the classic shell game. In a shell game, an item is placed beneath one of three identical shells, making that shell the target. The viewer tries to keep track of which shell has the item underneath it. -->
 <!-- insert Conversation article text-->


<!-- @nummenmaaCorticalCircuitBinding2017 found temporal lobe activation -->



<!--
In sport, however, sometimes we are suprised to discover an opponent in the right place at the right time. "Where did he come from?" is a common complaint. This phenomenon can be partially understood in terms of the limit on the number of objects that can be tracked in MOT. It turns out, however, that this is only part of the story. The limitations on tracking are more profound than those we have reviewed so far.

The tracking research we have discussed so far has used targets and distractors that are all identical to each other (after the initial phase where the targets are indicated). In the real world, however, this is uncommon.

Visual short-term memory tasks typically find that people can perform reasonably well at storing at least four objects, allowing participants to detect whether an object changes during a short interval. This, together with the misconception discussed in section \@ref(bottlenecks) that tracking has a limit of four targets has led many researchers to suggest that a common limitation underlies both visual short-term memory and object tracking. This may be true in some sense, but certain possibilities have been ruled out by a dramatic finding.
-->

<!--Putting objects in motion can dissociate tracking and short-term memory capacities. @saikiMultipleobjectPermanenceTracking2002 asked participants to track four colored discs that periodically moved behind occluders and then re-appeared on the other side. There were no distractors. The task of participants was to detect whether any discs changed color. Contrary to what would be expected from object files theory [@kahnemanReviewingObjectFiles1992], for all but very slow speeds, performance was very low. However, because distractors were not included, there was no direct comparison to performance tracking solely the objects' locations (without knowing their colors). -->




