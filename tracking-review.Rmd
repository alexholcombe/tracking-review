--- 
title: "How humans track objects"
author: "Alex O. Holcombe"
date: "`r paste0('Updated on ',Sys.Date())`"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    split_bib: FALSE
    config:
      sharing:
        twitter: yes
        facebook: no
documentclass: book
lof: yes
#(doesn't work) html_document:
# (doesn't work) code_folding: show

#Error: Functions that produce HTML output found in document targeting latex output.
#Please change the output type of this document to HTML. Alternatively, you can allow
#HTML output in non-HTML formats by adding this option to the YAML front-matter of your rmarkdown file:
always_allow_html: true
bibliography: [bibliography/CambridgeElement.bib, bibliography/CambridgeElementNewestAdditions.bib, bibliography/packages.bib]
#In Zotero, create the book.bib file by going to object_tracking->CambridgeElement, select all items, right-click->Export items->BetterBibTex
biblio-style: apalike
link-citations: yes
description: "Tracking-review"
url: 'https\://tracking.whatanimalssee.com/'
github-repo: alexholcombe/tracking-review
twitter-handle: ceptional
#For the order of chapters, see _bookdown.yml, where they are manually specified
---

# (doesn't work) code_folding: show

Placeholder



<!--chapter:end:index.Rmd-->


# Attending to moving objects

Placeholder


## Tracking task short description and anecdotes
## Outline

<!--chapter:end:01-outline.Rmd-->


# Bottlenecks and capacity {#bottlenecks}

Placeholder



<!--chapter:end:bottlenecks.Rmd-->


# The biggest myth of object tracking {#biggestMyth}

Placeholder


## Different tasks, same limit?

<!--chapter:end:theBiggestMyth.Rmd-->


# Objects and attentional spread

Placeholder


## Stationary objects and attentional spread
## The ends of objects
## Object creation and object tracking: Distinct processes?
## What tracking sticks to
## Growth, shrinkage, and tracking
## Could tracking work by spreading?
## Summing up

<!--chapter:end:objects.Rmd-->


# Grouping {#grouping}

Placeholder


## Hierarchical relations 
## Eyes to the center

<!--chapter:end:grouping.Rmd-->


# Which aspect(s) of tracking determine performance? {#whichAspects}

Placeholder


## General cognition (C≈1 processes)
## Duration that one can sustain attention
## Spatial selection of multiple locations
## Spatial interference
## Temporal interference
## Speed limit of attention-following
## Moving forward

<!--chapter:end:whichAspects.Rmd-->


# Spatial interference {#spatialInterference}

Placeholder


## Spatial interference does not explain why tracking many targets is more difficult than tracking only a few
## Spatial selection of multiple locations

<!--chapter:end:spatialInterference.Rmd-->

# The role of motion {#beyondLocation} 

The information that focused attention can deliver might conceivably benefit tracking for any target frequently focused on. One possible example is the direction and speed that a target is moving in. If this information is used to anticipate future positions of the objects, then performance should be better when objects maintain straight-line trajectories than when they frequently change their direction.

The evidence below suggests that the capacity limit on the use of motion information during tracking may be more severe than that on the use of position. That is, in conditions where participants can use position information to accurately track four or five targets, they may only use motion information for one or two of the targets. This may mean that the use of motion information can be identified with the extended cognitive processing of an object that likely can only occur for one or at most a few targets, which was referred to in section \@ref(whichAspects) as C≈1 processes. 

@howeMotionInformationSometimes2012 compared a condition in which the objects moved in straight lines, only changing direction when they bounced off the arena's boundaries, to when the objects' trajectories were not predictable because they changed direction randomly about every half second. However, this advantage for tracking the predictable trajectories was found when there were two targets, but not when there were four targets. @vulExplainingHumanMultiple2010 asked participants to track three targets and varied how much and how often the objects changed their velocity. They found little to no detriment of the velocity changes on participants' estimates of difficulty. Unfortunately they did not assess whether velocity continuity became beneficial with fewer targets.

In the @howeMotionInformationSometimes2012 experiments, participants were allowed to move their eyes. They may have moved their eyes to follow one target, or alternatively something like the centroid of the targets (see \@ref(grouping)), and as eye movements have some associated inertia, that tendency to continue moving the eyes in the same direction might have contributed to the predictable trajectory benefit, and it makes sense that this would boost conditions with fewer targets more given that the eyes only move in one direction at a time. @luuExtrapolationOccursMultiple2015 followed up on the @howeMotionInformationSometimes2012 results using similar experiment parameters but added a requirement that participants fixate at the center of the screen throughout a trial, and found a very similar pattern of results.

These results converge nicely with those of @fencsikRoleLocationMotion2007, who made targets invisible for a brief period (307 ms) during tracking. The targets continued moving, invisibly, during the disappearance interval, and participants were able to continue tracking afterward when there were one or two targets but not four targets, as evidenced by better performance compared to a control condition where prior motion information was not available.

@wangRoleKinematicProperties2021 devised displays that allowed them to compare the extent to which participants used position information, velocity, and acceleration during tracking. Consistent with previous investigations, velocity was used less than position, and was subject to a more severe capacity limit. Acceleration (extrapolation of change in motion direction) did not seem to be used at all.

## Velocity as a feature for correspondence matching?

For tracking, motion information could be used in two different ways. One is to solve what is referred to as the "correspondence problem". To understand this, imagine that the moving objects in an MOT display were sampled by a computer only once every three hundred milliseconds (something like this may be what the brain does when there are several targets, if attention samples objects serially - \@ref(serialOrParallel)). The correspondence problem is to determine the correspondence between the objects of the two frames. That is, solving the correspondence problem means knowing where an object in the first frame is in the second frame. The rise of CCTV a few decades ago sparked a rapid growth in the development of algorithms for tracking objects in low frame rate video [@kamkarMultipletargetTrackingHuman2020]. The correct answers for which objects in frames 1 and 2 correspond to each other can in many situations be obtained by nearest-neighbor matching. Nearest-neighbor here simply means matching each object in frame 1 to that closest to it in frame 2.

For some combinations of object trajectories and sampling frequencies, the nearest-neighbor match yields the wrong answer to the correspondence problem. For example, if in the interval between the two sampled frames, a distractor moving toward the target ends up very close to a target's location in frame 1, while the target has moved farther from its frame 1 location, then using nearest0neighbor will mistakenly match the target in frame 1 with a distractor in frame 2. This is called a "false correspondence".

Using nearest-velocity matching in conjunction can help avoid false correspondences. Velocity refers to both an object's direction and its speed. Because moving objects maintain their current velocity for a few hundred milliseconds or more, depending on the display, when two objects in frame 2 are both very close to the location a target occupied in frame 1, the target is likely to be the object whose velocity is most similar to the velocity of the target in frame 1. This topic will be discussed more in Chapter \@ref(resolution), because it relates to the motion streaks idea of that chapter..
<!-- has been suggested that a moving object leaves an extended trail that lingers in sensory memory and that tracking processes operate on that representation rather than objects' instantaneous positions  [@tripathyMultipleObjectTrackingSerial2011; @howardMultipleTrajectoryTracking2012].  -->

## Velocity for position estimation

The use of velocity matching to solve the correspondence problem must be distinguished from the topic of this section, using velocity to estimate position. A velocity signal can be used to predict or extrapolate the next position of a moving object. Consider a discrete sampling situation where one has a set of sensory signals of object locations on frames 1, 2, and 3. One can use the velocity signal for a target at frame 2 to extrapolate where it should be on frame 3. Then, when the sensory signals for frame 3 arrives, one can use the extrapolated target position as the input for solving the correspondence problem rather than the frame 2 position. I call this extrapolation of the present because if the brain uses this scheme, the idea is not that a person would perceive a moving object in a potential future position. Instead, the process is one of using the trajectory a target was on to help determine which new sensory signal corresponds to it.

The experiments reviewed in the first section of this chapter found evidence for the use of motion information, but that type of evidence could not distinguish between the use of motion for extrapolating the present and the use of motion for velocity matching. As we will see next, the results from two other paradigms find little to no evidence of extrapolation, which suggests that velocity matching is the way that motion information is used.

The first paradigm that was used to go looking for evidence of extrapolation, the "target recovery" paradigm, was developed by @keaneMotionExtrapolationEmployed2006. They had objects abruptly disappear during MOT and then reappear hundreds of milliseconds later. In their "move" conditions, they re-appeared further along the trajectory they would take had they continued with the same velocity, whereas in "non-move" conditions they would reappear in the same position they had disappeared in. Performance was uniformly worse in the move conditions than in the non-move conditions. A follow-up study by @franconeriSimpleProximityHeuristic2012 found the same result.

The possibility of extrapolation has also been explored by simply asking participants to report the last location of a target or targets after they disappear, by clicking with a mouse on the screen. If the brain extrapolates the present, that should result in participants reporting, on average, the correct last position of the target, although individual reports might be quite noisy. The brain might alternatively extrapolate the future, as has often been suggested (e.g. @nijhawanVisualPredictionPsychophysics2008), such that on average participants would click on a position ahead of a target's last position. Instead, studies have predominantly found that the locations participants report lag the final locations of the target, and this lag increases with the number of targets tracked [@howardTrackingChangingFeatures2008; @howardPositionRepresentationsLag2011]. One exception is from @iordanescuDemandbasedDynamicDistribution2009, who found that people clicked on average slightly ahead of the target's last position. However, @howardPositionRepresentationsLag2011 tried but failed to replicate this result, instead finding lags again. 

Further evidence that the visual system uses a lagged representation comes from an MOT eye-tracking study by @lukavskyGazePositionLagging2016. They were able to assess whether eye position either anticipated future positions of the objects or instead lagged their present position in an ingenious model-free way. They contrasted the eye movements in pairs of trials with object paths that were identical except that their trajectories were time-reversed. After reversing the timeline of the eye movement data from the backward trials, they time-shifted that data to find the time shift that maximized the correspondence of the eye movements for the two kinds of trials. In their first and second experiment, with four targets, the time-shifting technique of @lukavskyGazePositionLagging2016 revealed that eye movements lagged the targets in every participant, with a mean lag of 110 ms in the first experiment and 108 ms in the second experiment. 

Recall that a promising theory of tracking is that a process switches among the targets to update their positions - this would explain the dramatic worsening of temporal limits with additional targets reviewed in \@ref(speedAndTime). such a theory also entails that not only temporal limits, but also lags should worsen with additional targets. This is precisely what was found by @howardTrackingChangingFeatures2008 varied the number of targets from one to seven and found that the lag of the positions participants clicked on increased with the number of targets tracked. This supports a serial position sampling theory, as discussed in \@ref(serialOrParallel). A trend but *not* a statistically significant increase, however, was found by @howardPositionRepresentationsLag2011; they only varied the number of targets from one to three, and perhaps that was not enough. <!--@iordanescuDemandbasedDynamicDistribution2009 did not vary the number of targets to track. @corbettAttentionTwinkleGoes compared 1 vs. 4 and 1 vs. 2 targets and also didn't find sig. increase in lag, with either static or dynamic bg-->  @lukavskyGazePositionLagging2016 also investigated whether the lag changed with the number of targets, in their case the lag of eye position. In their second experiment numerically the mean lag was 15 ms less (93 ms) for two targets than for four, although again this was not a statistically significant difference - the 95% confidence interval spanned from 33 ms of lag to 2 ms of extrapolation. Thus while their results were compatible with the proposition that there is less lag with fewer targets, the data did not strongly support it. More work should be done in this area.

## Simulation evidence indicates that extrapolation has little value in MOT

The evidence of the preceding sections suggests that tracking processes do little in the way of extrapolation, or even velocity matching, except for when there are only a few targets, when more limited-capacity cognitive processes may play a larger role. The paucity of evidence for extrapolation is surprising in light of the popularity of predictive frameworks for conceptualizing what the brain does. Many researchers believe that prediction is a critical component of much of perception. So, is the brain leaving a lot of performance gains on the table by not using extrapolation when there are more than a few targets?

A computational investigation by @zhongWhyPeopleAppear2014 found that there is little to be gained by extrapolation in standard MOT tasks. @zhongWhyPeopleAppear2014 took an approach resembling what is often called an "ideal observer" approach. The idea is to build into a model the relevant properties of our sensory limitations and then assess how well an optimal algorithm for processing those signals would do, and investigate how it would be affected by task parameters. @zhongWhyPeopleAppear2014 did this by turning a Kalman filter loose on estimating object positions for use to solve the correspondence problem in MOT. In the term "Kalman filter", the word "filter" has a tendency to mislead people, as it is not a filter in the conventional sense. The Kalman filter is instead an algorithm that learns to estimate, in Bayesian fashion, the current position of the targets. Bayesian estimation is appropriate because the sensory estimates of the objects are not precise - the simulations of @zhongWhyPeopleAppear2014 assume that the sensory error is Gaussian-distributed, which is a reasonable approximation, although @zhongWhyPeopleAppear2014 also make various simplifying assumptions, such as that the Gaussian error has the same variance throughout the visual field.

The Kalman filter makes a prediction of the object's current position, based on its best estimate of the object's last position and its velocity. This prediction, based on previous sensory position signals and a velocity estimated from them, is combined with the current sensory position signal to yield the estimate of the object's current position. The relative weights assigned to the prediction and the sensory signal are determined by an updating process that arrives at the optimal weights under certain assumptions. 

@zhongWhyPeopleAppear2014 took the position estimates of the targets provided by the Kalman filter on each time step and used them to solve the correspondence problem. That is, rather than matching the sensory position data of the current frame to each sensory position datum from the previous frame believed to have been from a target, instead of this sensory data from the previous frame, they used the Kalman filter estimates of each target's position. @zhongWhyPeopleAppear2014 expected that simulated MOT task accuracy would be substantially higher when the Kalman filter was used, because the Kalman filter estimates of each target's position are substantially more accurate than the 'raw' sensory data.

To the surprise of the researchers, simulated MOT performance was not substantially higher for the Kalman filter than when the raw sensory data was used. This finding was robust to a range of parameter values for the simulation, so @zhongWhyPeopleAppear2014 concluded that extrapolation has very little benefit for the MOT tasks they investigated.

To understand this result, @zhongWhyPeopleAppear2014 suggested that one must first consider the situations that lead to errors in MOT. As we have suggested elsewhere in this book, most errors may arise during close encounters between targets and distractors. During the periods of an MOT trial when the targets and distractors are far from each other, there is no correspondence ambiguity and computational models such as that of @zhongWhyPeopleAppear2014 do not make mistakes, so extrapolation and velocity matching are certainly of no benefit there. During close encounters, by contrast, one might expect that extrapolation would reduce false correspondences. In their simulations, @zhongWhyPeopleAppear2014 found that extrapolation did reduce false correspondences, improving task performance but that this benefit was extremely small in size.

Why is there only a trivial benefit of extrapolation in the @zhongWhyPeopleAppear2014 simulations? False correspondences in the simulations are caused by noise in the incoming sensory position signals. The Kalman filter's representation is less noisy than the sensory signals, in part due to extrapolation, but the improvement in accuracy is dwarfed by the sensory noise, as far as resulting false correspondences. In other words, targets end up being swapped for distractors (false correspondence) largely due to the ambiguity in correspondence created by the sensory noise. This remained true for each of the different levels of sensory noise and intermittency of sampling that @zhongWhyPeopleAppear2014 simulated.

More work needs to be done with this sort of approach. @zhongWhyPeopleAppear2014 made some assumptions that are known to be false, such as that there is a uniform level of sensory noise across the visual field, and some that are implausible, such as that the brain can determine a global solution for the correspondence problem that minimizes the sum of the distances between the targets' position estimates provided by the Kalman filter and the new sensory observations. At least some of their results are probably robust to these assumptions, but possibly not all. 

## A C≈1 extrapolation effect?

Another behavioral paradigm in which participants report the last position of an object does frequently elicit evidence of extrapolation. In this "representational momentum" paradigm, participants are typically shown only a single moving object and asked to report the object's final position after it suddenly disappears. On average, participants usually indicate a position displaced in the object's final direction of motion. @hubbardRepresentationalMomentumRelated2005 provides an extensive review of a large literature on this. Participants were also found to displace the last position of a target in the direction of gravity. The phenomenon may reflect C=1 cognitive processes, but this remains uncertain because the number of objects is almost never varied in this literature. 

Another extrapolation phenomenon has been reported for frozen-action photographs that imply motion. For example, @freydMentalRepresentationMovement1983 presented a photograph such as of waves crashing on a beach, and participants judged whether a subsequently presented probe photograph was the same as the original photograph or different. The pattern of response time suggested that participants' memory of the photograph was closer to one from later in the series of photographs than the original.  @hafriMeltingIceYour2022 found that this form of extrapolation generalized to changes in state that could not easily be reduced to motion. For example, they found that an image of a burning log was remembered as being more burnt than it was in the original photograph.

In the literature, the term "representational momentum" is applied to both the extrapolations of the state of a stimulus like a burning log and the reported position of a moving object whose trajectory is abruptly terminated, although I don't know of strong evidence that these reflect the same phenomenon. However, it is plausible that these reflect a C≈1 process and thus would not show hallmarks of MOT such as hemifield independence. Because this sort of likely-cognitive or memorial process exists, researchers who are interested in the processes that underlie tracking should assess whether their findings can be explained by C≈1 processes before assuming that what they are studying is perceptual or attentional rather than cognitive. This issue is discussed further in the Recommendations section.

It is also possible that representational momentum is a memory effect, perhaps reflecting the same mechanisms that yield the boundary extension effect discovered by Helene Intraub [@intraubBoundaryExtensionFundamental1993]. However, @nakayamaDynamicNoiseBackground2021 recently discovered an extrapolation effect that appears to be perceptual, in that this "twinkle goes" illusion shows up with immediate report and is immediately perceived by many observers [in demonstrations](https://twitter.com/ceptional/status/1204116417429131264). The results from one experiment investigating this effect suggest that it is highly resource-intensive, however, because when attention was split across two targets the effect was greatly diminished. Possibly, then, the twinkle-goes illusion reflects a C≈1 effect, although more work is needed.

## Where does extrapolation stand?

<!-- When pursuing a single target with the eyes, @chenAttentionAllocatedClosely2017 used EEG evidence to show enhancement ahead of the target. But this could be due to eye movement planning -->


<!--I never realized that for short inter-target visits like with two targets, motion mechanisms may solve the correspondence problem, which. I never realized that the theory that predicts a linear increase in temporal limit assumes that the correspondence problem is solved like an ideal observer.
A new synthesis: the correspondence problem cannot be solved. But it could if we were extrapolating - circular trajectories would not be a problem, even if you used only the linear velocity rather than incorporating the curvature. Therefore we are not extrapolating. Are we solving the correspondence problem? If we weren't, then the motion should look degenerate, with wagon wheels happening and such. Which I think is exactly what MacDonald & vanRullen found with multiple wagon wheels? 
-->


<!--
has various benefits. In particular, top-down attentional selection, which is necessary for doing various mental operations on an object of interest, mainly uses location as the index for selecting objects (although color and a few other features may also be used).
together with color and a few other features used by "feature attention", is the main route by which top-down attentional selection occurs. But what benefits does attentional selection provide that are not already delivered by the selection associated with tracking?-->


TRANSITION TO MOBILE COMPUTATION.Rmd SOMEWHERE ON THIS PAGE

<!--
How well are other features besides position tracked? 

howardTrackingChangingFeatures2008 investigated tracking of spatial frequency and of orientation as well as of position.
Tracking the orientation of an object might conceivably be accomplished by tracking one end of the object, but @scholl showed that we are very poor at doing that.

One concern with this conclusion was that the results might be explained by a bottleneck on the number of spatial locations that participants needed to process rather than the number of locations. However, subsequent work was more effective at spatially overlapping two objects, which diminished this concern [@blaserTrackingObjectFeaturespace2000]. 

-->


<!-- The role of motion signals . Seiffert -->


<!--Extrapolation theory predicts 

* attention will be right on the target.
* Linear effect of velocity
-->

<!--
Atsma, J., Koning, A., & van Lier, R. (2012). Multiple object tracking: Anticipatory attention doesn’t “bounce.” Journal of Vision, 12(13), 1–1. Found anticipatory attention in the direction of the object movement. Only tested 3 targets and 1 target. Didn’t test the backward direction I don’t think to see whether that was even better!!!
-->




What about updating of features? Well, updating of surface features seems to generally be crap, e.g. identity tracking is crap, Pailian, Saiki

Integration: 
Relation to oscillations



<!--chapter:end:beyondLocation.Rmd-->


# Speed limits and temporal limits {#speedAndTime}

Placeholder


## A temporal limit on perception
## Temporal limits on visual cognition
## Low-level and high-level temporal limits {#loHighLevelLims}
## Temporal limits on tracking
## Temporal interference is highly resource-intensive
## Relation to other temporal limits
## Speed limits
## Putting it all together

<!--chapter:end:speedAndTime.Rmd-->


# Two brains or one? {#twoBrains}

Placeholder


## The extraordinary hemifield independence of object tracking
## Quantitative estimates of independence
## Some tracking resources are NOT hemifield-specific
## The underlying mechanisms
## What else are hemifield-specific resources used for?
## Hemispheric differences

<!--chapter:end:twoBrainsOrOne.Rmd-->


# Are positions sampled serially? {#serialOrParallel}

Placeholder


## Spatial relationships - a case study of serial processing 
## A case for serial position sampling
## Evidence for parallel processing
## Models of MOT

<!--chapter:end:serialOrParallel.Rmd-->


# Serial streak sampling {#resolution}

Placeholder


## Taking stock

<!--chapter:end:resolution.Rmd-->


# Mobile computation 

Placeholder


## Attentional tracking and mobile computation

<!--chapter:end:mobileComputation.Rmd-->


# Knowing where but not what {#identity}

Placeholder


## The first question: Does position updating benefit from differences in object identities?
### Motion correspondence 
### Feature differences, but not feature conjunction differences, benefit tracking
## The second question: Are we aware of the identities of objects we are tracking?
## Beaten by a bird brain
## Some dissociations between identity and location processing reflect poor visibility in the periphery
## Evidence from two techniques suggests parallel updating of identities
## Eye movements can add a serial component to tracking

<!--chapter:end:identity.Rmd-->


# Abilities, individual differences, dual-task interference {#abilities}

Placeholder


## Do people vary much in how many objects they can track?
## What tasks are most related to MOT? 
## Dual-task

<!--chapter:end:abilities.Rmd-->

# Real world

**Towards the real-world** and real-world tasks. A naive view of visual perception is that we are simultaneously aware of the identities of all the objects in a scene, so unless a player actually disappears or hides behind something or someone, we should know where everyone on the bacsketball court is at all times. Bringing together the factors described in previous sections, and bringing in new ones such as limits on feature binding, we will analyze some real-world tasks.

"further research is required to determine whether multiple-object tracking is actually used in day-to-day driving." @lochnerMultipleobjectTrackingDriving2014
 
 @mackenzieMultipleObjectAvoidance2021 used a multiple object avoidance task where the user used a mouse to control one of the balls that they had to prevent from colliding with the other balls. It's reminiscent of the old Asteroids video game that we opened this book with. Found decent correlations with performance on a driving simulator and with years of driving experience. In an earlier paper, some of these authors found that the MOA correlated better with driving performance than conventional MOT @mackenzieLinkAttentionalFunction2017 . @bowersCanWeImprove2013 found no correlation between MOT and  . there's one other paper.
 

@harrisExaminingRolesWorking2020 low N comparing sports players

@mangineVisualTrackingSpeed2014 Twelve professional basketball players were tested before the 2012–13 season. Visual tracking speed was obtained from 1 core session (20 trials) of the multiple object tracking test, whereas RT was measured by fixed- and variable-region choice reaction tests, using a light-based testing device. Performance in VTS and RT was compared with basketball-specific measures of performance (assists [AST]; turnovers [TO]; assist-to-turnover ratio [AST/TO]; steals [STL]) during the regular basketball season. All performance measures were reported per 100 minutes played. Performance differences between backcourt (guards; n = 5) and frontcourt (forward/centers; n = 7...

Study of only 12 pro basketball players finding correlations with basketball metrics @mangineVisualTrackingSpeed2014


<!--chapter:end:realWorld.Rmd-->


# Misconceptions and questions

Placeholder


## The decline of Pylyshyn's FINST theory
## Recommendations
## Topics not covered by this review

<!--chapter:end:misconceptionsAndQuestions.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:references.Rmd-->

