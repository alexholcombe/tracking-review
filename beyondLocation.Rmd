# The role of motion {#beyondLocation} 

Covert tracking keeps one up to date with the locations of objects of interest. Knowing objects' locations is sometimes important in itself. Also it allows us to quickly bring focused attention to one or more of the objects. When focused attention is applied to an object, that allows even our most limited-capacity processes to operate on the object, which seems to be necessary to, for example, make fine shape discriminations or recognize a word [@whiteVisualWordRecognition2020].

The information that focused attention can deliver might conceivably benefit tracking for any target frequently focused on. One possible example is the direction and speed that a target is moving in. If this information is used to anticipate future positions of the objects, then performance should be better when objects maintain straight-line trajectories than when they frequently change their direction.

The evidence below suggests that the capacity limit on the use of motion information during tracking may be more severe than that on the use of position. That is, in conditions where participants can use position information to accurately track four or five targets, they may only use motion information for one or two of the targets. This may mean that the use of motion information can be identified with the extended cognitive processing of an object that likely can only occur for one or at most a few targets, which was referred to in section \@ref(whichAspects) as C≈1 processes. 

@howeMotionInformationSometimes2012 compared a condition in which the objects moved in straight lines, only changing direction when they bounced off the arena's boundaries, to when the objects' trajectories were not predictable because they changed direction randomly about every half second. However, this advantage for tracking the predictable trajectories was found when there were two targets, but not when there were four targets. @vulExplainingHumanMultiple2009 asked participants to track three targets and varied how much and how often the objects changed their velocity. They found little to no detriment of the velocity changes on participants' estimates of difficulty. Unfortunately they did not assess whether velocity continuity became beneficial with fewer targets.

In the @howeMotionInformationSometimes2012 experiments, participants were allowed to move their eyes. They may have moved their eyes to follow one target, or alternatively something like the centroid of the targets (see \@ref(grouping)), and as eye movements have some associated inertia, that tendency to continue moving the eyes in the same direction might have contributed to the predictable trajectory benefit, and it makes sense that this would boost conditions with fewer targets more given that the eyes only move in one direction at a time. @luuExtrapolationOccursMultiple2015 followed up on the @howeMotionInformationSometimes2012 results using similar experiment parameters but added a requirement that participants fixate at the center of the screen throughout a trial, and found a very similar pattern of results.

These results converge nicely with those of @fencsikRoleLocationMotion2007, who made targets invisible for a brief period (307 ms) and participants attempted to continue tracking when they re-appeared after the interval in the locations expected if they had continued moving in the same direction and speed as when they disappeared. Participants were able to do this when there were one or two targets but not four targets, as evidenced by better performance compared to a control condition where prior motion information was not available.

@wangRoleKinematicProperties2021 devised displays that allowed them to compare the extent to which participants used position information, velocity, and acceleration during tracking. Consistent with previous investigations, velocity was used less than position, and was subject to a more severe capacity limit. Acceleration (extrapolation of change in motion direction) did not seem to be used at all.

## Velocity as a feature for correspondence matching?

For tracking, motion information could be used in two different ways. One is to solve what is referred to as the "correspondence problem". To understand this, imagine that the moving objects in an MOT display were sampled by a computer only once every three hundred milliseconds (something like this may be what the brain does when there are several targets, if attention samples objects serially - \@ref(serialOrParallel)). The correspondence problem is to determine the correspondence between the objects of the two frames. That is, solving the correspondence problem means knowing where an object in the first frame is in the second frame. This is not at all a theoretical problem - the rise of CCTV a few decades ago sparked a rapid growth in the development of algorithms for tracking objects in low frame rate video [@kamkarMultipletargetTrackingHuman2020]. The correct answers for which objects in frames 1 and 2 correspond to each other will in many cases be provided by the nearest-neighbor match. Nearest-neighbor here simply means matching each object in frame 1 to that closest to it in frame 2.

For some object trajectories, the nearest-neighbor match yields the wrong answer to the correspondence problem. For example, if in the interval between the two sampled frames, a distractor moving toward the target ends up very close to a target's location in frame 1, while the target has moved farther from its frame 1 location, then using nearest neighbor will mistakenly match the target in frame 1 with a distractor in frame 2. This is called a "false correspondence".

Using nearest-velocity matching in conjunction can help avoid false correspondences. Velocity refers to both an object's direction and its speed. Because moving objects maintain their current velocity for a few hundred milliseconds or more, depending on the display, when two objects in frame 2 are both very close to the location a target occupied in frame 1, the target is likely to be the object whose velocity is most similar to the velocity of the target in frame 1.

As was already mentioned in \@ref(serialOrParallel), it has been suggested that a moving object leaves an extended trail that lingers in sensory memory and that tracking processes operate on that representation rather than objects' instantaneous positions  [@tripathyMultipleObjectTrackingSerial2011; @howardMultipleTrajectoryTracking2012]. This is tantamount to using velocity as a feature for correspondence matching, although there does not appear to be any direct evidence for it. Still, there is evidence that the trails of moving objects, spanning about 100 ms of integration, do affect some aspects of perception [@apthorpSpatialTuningMotion2011].


@tripathyMultipleTrajectoryTracking2012 says people use trajectory information in tracking. He shows a severe capacity limit however for detecting a trajectory change.


## Velocity for position estimation

The use of nearest-velocity matching to solve the correspondence problem should be distinguished from using velocity to estimate position, in what I will call "extrapolation of the present". A velocity signal can be used to predict or extrapolate the next position of a moving object. Consider a discrete sampling situation where one has a set of sensory signals of object locations on frames 1, 2, and 3. One can use the velocity signal for a target at frame 2 to extrapolate where it should be on frame 3. Then, when the sensory signals for frame 3 arrives, one can use the extrapolated target position as the input for solving the correspondence problem rather than the frame 2 position. I call this extrapolation of the present because if the brain uses this scheme, the idea is not that a person would perceive a moving object in a potential future position. Instead, the process is one of using the trajectory a target was on to help determine which current sensory signal corresponds to it.

The experiments reviewed in the first section of this chapter found evidence for the use of motion information, but that type of evidence could not distinguish between the use of motion for extrapolating the present and the use of motion for velocity matching. As we will see next, the results from two other paradigms find little to no evidence of extrapolation, which suggests that velocity matching is the way that motion information is used.

The first paradigm that has been used to go looking for evidence of extrapolation, the "target recovery" paradigm, was developed by @keaneMotionExtrapolationEmployed2006. They had objects abruptly disappear during MOT and then reappear hundreds of milliseconds later. In their "move" conditions, they re-appeared further along the trajectory they would take had they continued with the same velocity, whereas in "non-move" conditions they would reappear in the same position they had disappeared in. Performance was uniformly worse in the move conditions than in the non-move conditions. A follow-up study by @franconeriSimpleProximityHeuristic2012 found the same result.

The possibility of extrapolation has also been explored by simply asking participants to report the last location of a target or targets after they disappear, by clicking with a mouse on the screen. If the brain extrapolates the present, that should result in participants reporting, on average, the correct last position of the target, although individual reports might be quite noisy. The brain might alternatively extrapolate the future, as has often been suggested (e.g. @nijhawanVisualPredictionPsychophysics2008), such that on average participants would click on a position ahead of a target's last position. Instead, studies have predominantly found that the locations participants report lag the final locations of the target, and this lag increases with the number of targets tracked [@howardTrackingChangingFeatures2008; @howardPositionRepresentationsLag2011]. One exception is from @iordanescuDemandbasedDynamicDistribution2009, who found that people clicked on average slightly ahead of the target's last position. However, @howardPositionRepresentationsLag2011 tried but failed to replicate this result, instead finding lags again. 

Further evidence that the visual system uses a lagged representation comes from an MOT eye-tracking study by @lukavskyGazePositionLagging2016. They were able to assess whether eye position either anticipated future positions of the objects or instead lagged their present position in an ingenious model-free way. They contrasted the eye movements in pairs of trials with object paths that were identical except that their trajectories were time-reversed. After reversing the timeline of the eye movement data from the backward trials, they time-shifted that data to find the time shift that maximized the correspondence of the eye movements for the two kinds of trials. In their first and second experiment, with four targets, the time-shifting technique of @lukavskyGazePositionLagging2016 revealed that eye movements lagged the targets in every participant, with a mean lag of 110 ms in the first experiment and 108 ms in the second experiment. 

Recall that a promising theory of tracking is that a process switches among the targets to update their positions - this would explain the dramatic worsening of temporal limits with additional targets reviewed in \@ref(speedAndTime). such a theory also entails that not only temporal limits, but also lags should worsen with additional targets. This is precisely what was found by @howardTrackingChangingFeatures2008 varied the number of targets from one to seven and found that the lag of the positions participants clicked on increased with the number of targets tracked. This supports a serial position sampling theory, as discussed in \@ref(serialOrParallel). However, a statistically significant increase with number of targets was not found by @howardPositionRepresentationsLag2011; they only varied the number of targets from one to three, and perhaps that was not enough. <!--@iordanescuDemandbasedDynamicDistribution2009 did not vary the number of targets to track. @corbettAttentionTwinkleGoes compared 1 vs. 4 and 1 vs. 2 targets and also didn't find sig. increase in lag, with either static or dynamic bg-->  @lukavskyGazePositionLagging2016 also investigated whether the lag changed with the number of targets, in their case the lag of eye position. In their second experiment numerically the mean lag was 15 ms less (93 ms) for two targets than for four, but this was not a statistically significant difference - the 95% confidence interval spanned from 33 ms of lag to 2 ms of extrapolation. Thus while their results were compatible with the proposition that there is less lag with fewer targets, the data did not strongly support it. More work should be done in this area.

## Simulation evidence indicates that extrapolation has little value in MOT

The evidence of the preceding sections suggests that tracking processes do little in the way of extrapolation, or even velocity matching, except for when there are only a few targets, when more limited-capacity cognitive processes may play a larger role. The paucity of evidence for extrapolation is surprising in light of the popularity of predictive frameworks for conceptualizing what the brain does. Many researchers believe that prediction is a critical component of much of perception. So, is the brain leaving a lot of performance gains on the table by not using extrapolation when there are more than a few targets?

A computational investigation by @zhongWhyPeopleAppear2014 found that there is little to be gained by extrapolation in standard MOT tasks. @zhongWhyPeopleAppear2014 took an approach resembling what is often called an "ideal observer" approach. The idea is to build into a model the relevant properties of our sensory limitations and then assess how well an optimal algorithm for processing those signals would do, and investigate how it would be affected by task parameters. @zhongWhyPeopleAppear2014 did this by turning a Kalman filter loose on estimating object positions for use to solve the correspondence problem in MOT. In the term "Kalman filter", the word "filter" has a tendency to mislead people, as it is not a filter in the conventional sense. The Kalman filter is instead an algorithm that learns to estimate, in Bayesian fashion, the current position of the targets. Bayesian estimation is appropriate because the sensory estimates of the objects are not precise - the simulations of @zhongWhyPeopleAppear2014 assume that the sensory error is Gaussian-distributed, which is a reasonable approximation, although @zhongWhyPeopleAppear2014 also make various simplifying assumptions, such as that the Gaussian error has the same variance throughout the visual field.

The Kalman filter makes a prediction of the object's current position, based on its best estimate of the object's last position and its velocity. This prediction, based on previous sensory position signals and a velocity estimated from them, is combined with the current sensory position signal to yield the estimate of the object's current position. The relative weights assigned to the prediction and the sensory signal are determined by an updating process that arrives at the optimal weights under certain assumptions. 

@zhongWhyPeopleAppear2014 took the position estimates of the targets provided by the Kalman filter on each time step and used them to solve the correspondence problem. That is, rather than matching the sensory position data of the current frame to each sensory position datum from the previous frame believed to have been from a target, instead of this sensory data from the previous frame, they used the Kalman filter estimates of each target's position. @zhongWhyPeopleAppear2014 expected that simulated MOT task accuracy would be substantially higher when the Kalman filter was used, because the Kalman filter estimates of each target's position are substantially more accurate than the 'raw' sensory data.

To the surprise of the researchers, simulated MOT performance was not substantially higher for the Kalman filter than when the raw sensory data was used. This finding was robust to a range of parameter values for the simulation, so @zhongWhyPeopleAppear2014 concluded that extrapolation has very little benefit for the MOT tasks they investigated.

To understand this result, @zhongWhyPeopleAppear2014 suggested that one must first consider the situations that lead to errors in MOT. As we have suggested elsewhere in this book, most errors may arise during close encounters between targets and distractors. During the periods of an MOT trial when the targets and distractors are far from each other, there is no correspondence ambiguity and computational models such as that of @zhongWhyPeopleAppear2014 do not make mistakes, so extrapolation and velocity matching are certainly of no benefit there. During close encounters, by contrast, one might expect that extrapolation would reduce false correspondences. In their simulations, @zhongWhyPeopleAppear2014 found that extrapolation did reduce false correspondences, improving task performance but that this benefit was extremely small in size.

Why is there only a trivial benefit of extrapolation in the @zhongWhyPeopleAppear2014 simulations? False correspondences in the simulations are caused by noise in the incoming sensory position signals. The Kalman filter's representation is less noisy than the sensory signals, in part due to extrapolation, but the improvement in accuracy is dwarfed by the sensory noise, as far as resulting false correspondences. In other words, targets end up being swapped for distractors (false correspondence) largely due to the ambiguity in correspondence created by the sensory noise. This remained true for each of the different levels of sensory noise and intermittency of sampling that @zhongWhyPeopleAppear2014 simulated.

More work needs to be done with the ideal observer approach. @zhongWhyPeopleAppear2014 made some assumptions that are known to be false, such as that there is a uniform level of sensory noise across the visual field, and some that are implausible, such as that the brain can determine a global solution for the correspondence problem that minimizes the sum of the distances between the targets' position estimates provided by the Kalman filter and the new sensory observations. Probably their results would be robust to some of these assumptions, but possibly not all. 

## A cognitive extrapolation effect?

Another behavioral paradigm in which participants report the last position of an object does frequently elicit evidence of extrapolation. In this "representational momentum" paradigm, participants are typically shown only a single moving object and asked to report the object's final position after it suddenly disappears. On average, participants usually indicate a position displaced in the object's final direction of motion. @hubbardRepresentationalMomentumRelated2005 provides an extensive review of the large literature. Displacement has also been found in the direction of gravity. The phenomenon may reflect C=1 cognitive processes, but this remains uncertain because the number of objects is almost never varied in this literature. 

Another extrapolation phenomenon has been reported for frozen-action photographs that imply motion. For example, @freydMentalRepresentationMovement1983 presented a photograph such as of waves crashing on a beach, and participants judged whether a subsequently presented probe photograph was the same as the original photograph or different. The pattern of response time suggested that participants' memory of the photograph was closer to one from later in the series of photographs than the original.  @hafriMeltingIceYour2022 found that this form of extrapolation generalized to changes in state that could not easily be reduced to physical motion. For example, they found that an image of a burning log was remembered as being more burnt than it was in the original photograph.

In the literature, the term "representational momentum" is applied to both the extrapolations of the state of a stimulus like a burning log and the reported position of a moving object whose trajectory is abruptly terminated, although I don't know of strong evidence that these reflect the same phenomenon. However, it is plausible that these reflect a C≈1 process and thus would not show hallmarks of MOT such as hemifield independence. Because this sort of likely-cognitive or memorial process exists, researchers who are interested in the processes that underlie tracking should assess whether their findings can be explained by C≈1 processes before assuming that what they are studying is perceptual or attentional rather than cognitive. This issue is discussed further in the Recommendations section.

## Where does extrapolation stand?

" serial selection provides access to other object features, such as a target’s motion history" @lovettSelectionEnablesEnhancement2019

When pursuing a single target with the eyes, @chenAttentionAllocatedClosely2017 used EEG evidence to show enhancement ahead of the target




<!--I never realized that for short inter-target visits like with two targets, motion mechanisms may solve the correspondence problem, which. I never realized that the theory that predicts a linear increase in temporal limit assumes that the correspondence problem is solved like an ideal observer.
A new synthesis: the correspondence problem cannot be solved. But it could if we were extrapolating - circular trajectories would not be a problem, even if you used only the linear velocity rather than incorporating the curvature. Therefore we are not extrapolating. Are we solving the correspondence problem? If we weren't, then the motion should look degenerate, with wagon wheels happening and such. Which I think is exactly what MacDonald & vanRullen found with multiple wagon wheels? 
-->


<!--
has various benefits. In particular, top-down attentional selection, which is necessary for doing various mental operations on an object of interest, mainly uses location as the index for selecting objects (although color and a few other features may also be used).
together with color and a few other features used by "feature attention", is the main route by which top-down attentional selection occurs. But what benefits does attentional selection provide that are not already delivered by the selection associated with tracking?-->



When one needs to know more information about the objects,

one wants to scrutinize the objects further

Beyond simple location information, it is  advantageous to
But beyond this location information, what benefits does it provide? 
what benefits does it provide  

TRANSITION TO MOBILE COMPUTATION.Rmd SOMEWHERE ON THIS PAGE

maintain an up-to-date representation of where 

How well are other features besides position tracked? 

howardTrackingChangingFeatures2008 investigated tracking of spatial frequency and of orientation as well as of position.
Tracking the orientation of an object might conceivably be accomplished by tracking one end of the object, but @scholl showed that we are very poor at doing that.

One concern with this conclusion was that the results might be explained by a bottleneck on the number of spatial locations that participants needed to process rather than the number of locations. However, subsequent work was more effective at spatially overlapping two objects, which diminished this concern [@blaserTrackingObjectFeaturespace2000]. 



The role of motion signals . Seiffert

Extrapolation theory predicts 

* attention will be right on the target.
* Linear effect of velocity

Lack of extrapolation:

@howardPositionRepresentationsLag2011 included a review of the literature on keeping up with current position.  Eye movements lag -  @lukavskyGazePositionLagging2016


Atsma, J., Koning, A., & van Lier, R. (2012). Multiple object tracking: Anticipatory attention doesn’t “bounce.” Journal of Vision, 12(13), 1–1. Found anticipatory attention in the direction of the object movement. Only tested 3 targets and 1 target. Didn’t test the backward direction I don’t think to see whether that was even better!!!




Ryo Nakayama attention inertia theory

What about updating of features? Well, updating of surface features seems to generally be crap, e.g. identity tracking is crap, Pailian, Saiki

Integration: 
Relation to oscillations


