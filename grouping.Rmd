# Grouping {#grouping}

Carving the scene into objects is not the only segmentation task conducted by our visual systems. We also perceive groups, which can be defined by a variety of cues. Can tracking follow entire groups of unconnected objects? @alzahabiEnsemblePerceptionMultipleobject2021 used clusters of discs as targets and as distractors. These clusters maintained a constant spatial arrangement as they wandered about the display. Participants seemed to do well at tracking these clusters. However, these researchers did not rule out the possibility that participants were tracking just one disc of each cluster, and I am not aware of any work providing good evidence that a tracking focus can track an entire group.

@yantisMultielementVisualTracking1992 hypothesized that in MOT experiments, participants track an imaginary shape formed by the targets, specifically a polygon whose vertices are the target positions. This became a popular idea, but progress has been slow in understanding whether all participants do this or just a minority do, and in what circumstances. @merkelSpatiotemporalPatternsBrain2014 found a result that they took as evidence that some participants track a shape defined by the targets while others do not. In their task, at the end of the trial when the targets and distractors stopped moving, four of the objects were highlighted and the task was to press one button if all four were targets (match), and to press a different button otherwise (non-match). Error rates were lowest when none of the objects highlighted were targets, and errors were progressively more common as the number of highlighted objects that were targets increased. This was unsurprising. For trials where all four of the highlighted objects were targets (match), however, error rates were much lower than when only three were targets (a non-match). @merkelSpatiotemporalPatternsBrain2014 suggested that this reflected a "perceptual strategy of monitoring the global shape configuration of the tracked target items." They went on to split the participants based on whether they had a relatively low error rate in the match condition, investigated the neural correlates of that, and made various conclusions about the neural processing that underlies virtual shape tracking.

<!--The inferences of @merkelSpatiotemporalPatternsBrain2014 are based on the split of participants based on low error rate in the match condition compared to the condition where none of the highlighted objects match.--> The idea seems to be that if participants weren't using a shape tracking strategy, error rates would steadily increase from the trials where none of the highlighted objects were targets to the trials where all of the objects highlighted were targets. However, there are other possible reasons for this pattern of results. It does seem likely that participants use the shape at the end of the trial to rapidly classify the trial as match or non-match. People can certainly see shapes defined by their vertices by dots, as when the ancients looked at the sky and saw constellations of stars as animals and human figures. Subitizing is a related ability that is much faster than considering individual dot positions. So using shape is indeed a natural way to check for a match at the end. However, that doesnâ€™t mean much about whether the participants are using shape during the trial to track, when the shape is constantly deforming.

The low error rate in the full-match condition might also occur for other reasons besides grouping. Imagine that a participant tracked only one target and checked that target at the end of each trial for whether it was highlighted. If that one target is not highlighted, the participant presses the non-match button, otherwise they press the match button. For such a participant, the chance of getting the answer wrong when none of the probed objects are targets is zero. It is higher when one of the probed objects is a target (25% if the participant tracks one target perfectly on every trial and makes no finger errors), and still higher when two or three probed objects is a target. When three probed objects are targets, in three out of four cases, one of them is the target the participant tracked, so the participant frequently gets it wrong. But when all four of the probed objects are targets, the participant will always respond correctly. Thus, a low error rate in this all-targets-probed condition that is very close to the error rate in the no-targets-probed condition can be a sign of a participant who only monitors one target. But @merkelSpatiotemporalPatternsBrain2014 interpreted this result as instead meaning that the participant was tracking a virtual shape formed by all four of the targets. Another sort of evidence for whether people track virtual shapes is needed. Some evidence against the virtual shape tracking account is mentioned in the last section of this chapter, below.

<!--This doesn't address their correlation argument. Correlation predictions. Simplest version of my account is that only lapse rate differs between participants. High lapse rate contributes most to M0 and M4 variance, so if lapse rate is driving individual differences, correlation between M4 and M0 should be higher than M4 with other conditions, which it is. But for M1 and M2 I'm not sure why correlations so high.

Does Pearson correlations not affected by base rate for an error rate variable? I think not, so if there is a floor effect for M4, it should have a smaller correlation with everything else. But actually M0- has the lowest error rate and it has higher correlations.

What if you track three targets? Decision rule: if all three match, say yes, otherwise no. Then in M4 you will get 0% error rate again. With M3, 25%? error rate, then smaller for M2, M1, M0.  

Why would you ever say 'no' in M4? It would have to be because you ended up tracking the wrong target(s). But if you know you sometimes end up tracking a wrong target(s), if 3 of your 4 targets match 3 of those highlighted, then maybe your decision rule is to say 'yes'. So then sometimes you get M4 wrong. And in M3, you could get even more wrong, because you are happy with just 3 being the same. In M2, you might get less wrong.

Participants could differ in A) lapse rate; B) number tracked; C) decision rule.
They claim that half of participants have a big difference . But these might be the non-holistic participants. If I track holistically, with some noise in my virtual polygon representation I get M4 most correct? I get M3 most wrong, M2 less wrong, M1 even less, and M0 all correct.

-->


<!--Imagine you just check one target on every trial for whether it is highlighted. Chance of getting it wrong is zero at M0 ,progressively higher for M1, M2, and M3. But then at M4, should go back to zero. So this effect is unsurprising. And if you select people with low error rates for M4, you might be grabbing people who only tracked one target. But Merkel probably think they're grabbing people who tracked holistically.

They correlated M4 with everything else, and those correlations were lower than M1 with M2, M2 with M3, etc. Whereas all participants responded rapidly and with a low error rate on the match-0 trials, about half of the participants also responded rapidly and with a low error rate on the full-match trials. These latter participants also tended to make more errors on match-3 trials than did the other half of the participants.
-->


## Hierarchical relations 

In the real world, the movement of object images on our retinas is rarely as independent as the movement of the objects in an MOT experiment. In the real world, often there is a strong motion element throughout the visual field created by the movement of the observer, and recovering true object movement involves detecting deviations from that overall motion [@warrenPerceptionObjectTrajectory2007]. Even when the observer and their eyes do not move, hierarchical motion relationships are common. When one views a tree on a windy day, the largest branches sway slowly, while the smaller limbs attached to the larger branches move with the larger branches but also, being more flexible and lighter, have their own, more rapid motion.

These aspects of the structure of the visual world may be one reason that our visual systems are tuned to relative motion [@tadinWhatConstitutesEfficient2002; @maruyaRapidEncodingRelationships2013]. When we see a wheel roll by, we experience individual features on the wheel as moving forward, reflecting the global frame of the entire wheel, but also as moving in a circle, reflecting the motion relative to the center of the wheel.

This decomposition of a wheel rim's movement is so strong that people systematically mis-report the trajectory of the points on the wheel [@proffittUnderstandingWheelDynamics1990]. The red curve in the animation below reveals that a point on a rolling wheel traces out a curve that involves up, down, and forward motion, but no backward motion. When asked what they see, however, people report that they see circular motion, including an interval when the point moves backwards. This highlights that what we perceive reflects a sophisticated parsing and grouping of retinal motion signals.

```{r cycloid, echo=FALSE, out.width="30%", fig.cap="The red curve is that traced out by a point on a rolling wheel, by Zorgit https://commons.wikimedia.org/wiki/User:Zorgit"} 
#Somehow including the file part of the credit in the quotations above (https://commons.wikimedia.org/wiki/File:Cycloid_f.gif) breaks the PDF output, so I had to delete that and just link to the user.
if (knitr::is_html_output())   knitr::include_graphics("movies/cycloid/Cycloid_f.gif") else knitr::include_graphics("movies/cycloid/Cycloid_f_static.png")
```

@billHierarchicalStructureEmployed2020 varied the structured motion pattern of the moving discs of an MOT task to show that hierarchical relations in the stimulus can facilitate tracking. Unfortunately, they did not investigate whether those relations affected how the discs' motion was perceived, like in a rolling wheel or a flock of birds. The attentional demands, if any, of such hierarchical motion decomposition has not been explored much<!--FUTURE-->. Thus it remains unclear to what extent the hierarchical relations are calculated by the application of tracking or other attentional resources, versus tracking operating on a representation of hierarchical relations that was determined pre-attentively.

<!-- Mention the bendy-pencil illusion https://jov.arvojournals.org/article.aspx?articleid=2193187 maybe.  "the illusory bending motion may be due to an inability of observers to accurately track the motions of features whose image displacements undergo rapid simultaneous changes in both space and time" -->

## Eyes to the center

The human visual system does not represents scenes as just an atomized collection of objects. It also performs a rapid global analysis of visual scenes, providing summary information sometimes referred to as "ensemble statistics" [@alvarezSpatialEnsembleStatistics2009]. One such ensemble statistic is the location of the center or centroid of a set of objects. This is useful for eye movement planning, among other things â€” to monitor a group of objects, it is helpful to look at the center of the group, as that can minimize how far into peripheral vision the objects are situated. 

@zelinskyEyeMovementAnalysis2008 and @fehdEyeMovementsMultiple2008 both reported that during multiple object tracking, the eyes of many participants frequently are directed at blank locations near the center of the array of targets. This finding has been replicated by subsequent work [@hyonaEyeBehaviorMultiple2019]. The nature of the central point that participants tend to look at (in addition to the individual targets, which they also look at) is not entirely clear. Researchers have suggested that the point may be the average of the targets' locations, or the average location of all the moving objects (both targets and distractors). Another possibility that has been investigated is that participants tend to look at the centroid of the *shape* formed by the targets, which recalls the @yantisMultielementVisualTracking1992 hypothesis that what is tracked is the shape defined by the targets. @lukavskyEyeMovementsRepeated2013 introduced the idea of an "anti-crowding point", which minimizes the ratio between each target's distance from the gaze point and distance from every distractor. The idea was that participants move their gaze closer to a target when it is near a distractor to avoid confusing targets with distractors. Note, however, that the Lukavsky metric does not take into account the limited range of the empirical crowding zone, which is about half the eccentricity of an object.

In a comparison of several metrics against the eyetracking data, @lukavskyEyeMovementsRepeated2013 found that the anti-crowding point best predicted participants' gaze in his experiment, followed by the average of the target locations. These points both matched the data better than the centroid of the targets. This undermines the @yantisMultielementVisualTracking1992 hypothesis that a virtual polygon is tracked, and the finding of best performance for the anti-crowding point is consistent with other results that participants tend to look closer at targets that are near other objects 
[@vaterDisentanglingVisionAttention2017; @zelinskyRoleRescueSaccades2010].

More work must be done to understand the possible role of the anti-crowding eye movement strategy suggested by @lukavskyEyeMovementsRepeated2013. Spatial interference does not seem to extend further than half an object's eccentricity, in both static identification tasks [@pelliUncrowdedWindowObject2008' @gurnseyCrowdingSizeEccentricity2011] and multiple object tracking [@holcombeObjectTrackingAbsence2014], but the anti-crowding point devised by @lukavskyEyeMovementsRepeated2013 does not incorporate such findings. Its performance could be compared to a measure that is similar but excludes from the calculation distractors further than about half an object's eccentricity.

