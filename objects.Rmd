# Objects and attentional spread {#objects}

Sunlight from the window illuminates dozens of objects in my living room, including Hugo, our family dog, who lies napping in the corner. As Hugo stands up and ambles into the kitchen, I begin to track him with my attention. To do so, something in my brain has to identify a changing set of neurons as representing a single object - the conglomeration that is Hugo as his image shuffles across my retina. How is this done?

Plenty of processing occurs in the retina, but it is far from enough to segment an object from the background presented by my cluttered living room. That requires further processing in the thalamus and visual cortex. Some of this processing occurs regardless of where one is attending â€” in other words, it is pre-attentive. Exactly how extensive this preattentive processing is, and what sorts of representations it results in, is  not fully understood [@neisserDecisionTimeReactionTimeExperiments1963; @treismanVerbalCuesLanguage1964; @kimchiFiguregroundSegmentationCan2008].

Attentive tracking is typically discussed as if an enitrely preattentively-created representation is simply selected. It is unclear, however, whether processing is so neatly divided, with preattentive representations simply selected rather than attention participating in modifying or even creating the representation that is tracked (see Figure \@ref(fig:simpleArchitecture)). Quite popular already is that attending to a location results in binding some of the features there, such as color and orientation, but attention may also contribute to figure-ground segregation, which is fundamental to defining an object [@petersonLowlevelHighlevelContributions2014]. <!--As another intriguing possibility for the effect of attention, to explain the twinkle-goes illusion @nakayamaDynamicNoiseBackground2021 suggested that attentional tracking could cause the representation of a moving object to persist after the object has disappeared.-->

<!-- This selection process than allows for continuous monitoring of that entity's changing position as well as other sorts of events, such as detection of probes flashed on the associated stimulus. -->

Studying the relationship of tracking to preattentive processing can be difficult. Something that is both relevant and can easily be studied is which sorts of stimuli can be tracked and which cannot. A small complication, however, is that the initial deployment of attention likely occurs more via a spatial or featural index than through some sort of index of the objects in the scene. We cannot think "car" or "tree" to ourselves and expect our attention to immediately deploy to any cars or trees in the scene. In contrast, our ability to deploy attention to the cued, static *location* is well-established, which very rapidly facilitates perceptual performance for that location and neural activation in the associated parts of retinotopic cortices. We can also deploy attention directly to certain other features, such as color or motion direction  [@saenzGlobalFeaturebasedAttention2003; @whiteFeaturebasedAttentionInvoluntarily2011].

As a result of our featural selection capability, if a moving target differs from distractors in certain ways, then featural selection can be relied on to keep attention on the moving target. For example, if the targets are the only yellow objects in the scene, and all the distractors are blue or green, then one can think "yellow" and that will keep attention on the targets and off the distractors (see also Chapter \@ref(identity)). It is only when the targets are identical to the distractors, or not distinguishable from the distractors by one of the features that feature selection acts on, that a different process is needed to keep attention on a moving target.

With identical targets and distractors, spatial location selection may initiate the selection of a target, but if that were the only process operating, when an object moved, attention would be left behind. Strikingly, this almost never seems to happen. My experience of tracking is that the movement of attention along with a moving object typically seems to take no more effort than attending to a static object. Attention seems to be positively pulled along - when the targets in an MOT trial begin to move, I have never had the experience of my attention staying behind, remaining at one of the original target locations. Indeed, it feels completely unnatural to un-latch attention from a target and fix it to the target's current location while the target moves on. This likely is related to the ability of visual transients to attract attention - a moving object is essentially a rapid sequence of transients that appear along a path.

<!--"Object-based attention" has been invoked as an explanation of why attention seems to automatically move along with a selected object, the idea being that the units of attentional selection are objects rather than locations [@pylyshynSeeingVisualizingIt2006; @clarkLocationLocationLocation2009].-->  

## Stationary object selection

To access an object, I think that attention is deployed first to a location or locations, via spatial or featural cuing -  while researchers often speak about "object-based attention", no one seems to think that direct selection of objects is a thing, in the way that color selection is. That is, one cannot think "chair" and have all the locations of chairs in the scene become rapidly attended. Selection of chairs, or another object type, seems to require a search first, based on locations and simpler features. And even color selection may work via location, with thinking of a color resulting in availability of its locations, and attention then being deployed to those locations [@shihThereFeaturebasedAttentional1996].

While selection may begin with a location, the presence of an object in a location may result in spatial attention spreading throughout that object, which ultimately sets the stage for tracking that object. This "attentional spread" theory is a popular one, in part due to the evidence from the paradigm I will describe next.

Dozens, possibly hundreds, of published papers have investigated the relationship of objects to attention using a cuing paradigm developed by @eglyShiftingVisualAttention1994.  @eglyShiftingVisualAttention1994 presented two static objects (rectangles) and presented a cue on one end of an object or another. They found that the cue resulted in a performance enhancement not only for probes at the location of the cue, but also at the cued object's other end. The control or baseline condition was performance at locations equidistant from the cue but not on the cued object. The findings reported in most of the many follow-up papers similarly find that participants are fastest and most accurate when the stimulus is presented in the same location as the cue, or on the same object but on a different part of that object. I used the qualifier "most" when referring to the follow-up papers because some papers did not find this [@davisReversalObjectBased2005; @shomsteinObjectbasedAttentionStrength2008; @shomsteinObjectbasedAttentionSensory2002; @louIndividualDifferencesTemporal2020], and a major concern is that there may be many more such null findings that ended up in the proverbial file drawer. Indeed, the effect sizes in the literature are often quite small and the studies not highly powered, which can be a red flag that publication bias may have created the illusion of a real effect [@buttonPowerFailureWhy2013].

Based on the pattern of sample sizes, effect sizes, and p-values in three dozen published object-based attention studies, @francisExcessSuccessArticles2022 argued that publication bias and/or p-hacking in the literature is rife. This is plausible in part because substantial proportions of researchers in psychology and other fields admit to such practices [@johnMeasuringPrevalenceQuestionable2012; @rabeloQuestionableResearchPractices2020; @chinQuestionableResearchPractices2021]. @francisExcessSuccessArticles2022 further point out that the only previously-published study with a large sample (120 participants) found a non-significant effect, of only a 6.5 ms response time advantage [@pilzHowPrevalentObjectBased2012], and in Francis et al.'s own study with 264 participants, the effect was also quite small, at 14 ms. For an effect of this size, Francis et al. calculated that the sample sizes typically used in the published literature were unlikely to yield statistical significance without some help from p-hacking or another questionable research practice. As a result, many papers in the literature make conclusions about objects and attention based on results that unfortunately cannot be trusted.

Publication bias and p-hacking are less of an issue when the effects being studied are large, because in those cases studies are more likely to be adequately powered, resulting in fewer false positives and fewer false negatives. Some effects are so large that just seconds of looking at a display is enough to convince oneself that an effect is real. Fortunately, those large effects include some that relate to how variation in objects affects tracking, as we will see in the next section.

## The end of the line

Many objects, such as a letter 'T', have salient parts. While normally we think of 'T' as a single object, we can also see that it is made up of a horizontal segment and a vertical segment. In conscious awareness, then, we have access to both the whole object level and to an individual parts level. You are able to focus attention on individual bits of the vertical segment, even though there are no visual characteristics that differentiate it. But what kind of representation(s) does our object tracking processes operate on?

In early visual cortex, different populations of neurons respond to the the horizontal and to the vertical stroke of a 'T', as well as the different ends of each stroke. But having neurons that respond to a thing does not suffice to be able to track that thing, as tracking operates on only some sorts of representations. @schollWhatVisualObject2001 asked participants to try to track the ends of lines. This may seem like a weird task, but it is not entirely unrealistic. When paying attention to someone holding a rifle, for example, it may be important to continuously monitor the location of the front of its barrel. Four moving lines were presented in the @schollWhatVisualObject2001 study, with one end of each line designated as a target. At the end of the trial, the lines stopped moving and participants were to click with a mouse on the line ends that were targets. During a trial, each line grew, shrank, and rotated as each of its ends wandered about the screen randomly.

![A schematic of the display used by Scholl et al. (2001)](imagesForRmd/linesSchollPylyshynFeldman_madeByHolcombe.png){width=40%}

The results were striking. Performance on the task was very poor by any measure, including relative to a control condition in which the two ends of the line were not connected. Indeed, simply by viewing an example trial, one very quickly gets a sense of how difficult the task is.


```{r lines, echo=FALSE, out.width="100%", fig.cap="Using this display, Scholl et al. (2001) asked participants to track the end of each of several lines."}
#Work-around to make GIFs (but not .mov) work by avoiding including them in non-html outputs!
#https://stackoverflow.com/questions/64038037/can-i-conditionally-exclude-some-elements-code-blocks-from-rendering-to-the-pd
if(knitr::is_html_output()) knitr::include_url("movies/MOTmovies/connectedTargets/MOTtrackingEndOfLinesVeryDifficultScholl.mov") else knitr::include_graphics("movies/MOTmovies/connectedTargets/MOTtrackingEndOfLinesOneFrame.png")
#, height = "250px"
```

The task of tracking line ends in the @schollWhatVisualObject2001 experiment was complicated by the fact that the objects frequently crossed over each other, and also their length changed over time. But @howeCanAttentionBe2012 showed that these complications were not the main reason for the poor performance. It simply is the case, it seems, that one cannot confine one's tracking processes to one bit of an undifferentiated object. This inability to track the ends of lines fits in with a view already mentioned, that preattentive processes define objects, or at least proto-objects, and this is what tracking operates on.

Maintaining attention on a part of the visual scene in the absence of anything in the image to delineate that part feels like it requires concentration, as if we must continually think about what we are supposed to be attending to. If cognitive, "System 2" resources are indeed needed to maintain the "object" representation when it is not provided by preattentive processes, then for such objects we may only be able to track one. This idea that processes with a capacity of 1 are involved or required for some forms of tracking was introduced in Chapter \@ref(Cequals1). 

## Object creation and object tracking: Distinct processes?

Many researchers distinguish between the processing that determines *how many* objects one can track and those that determine *what kinds* of objects can be tracked. An assumption of separate processing stages is popular in the study of visual cognition quite generally. Visual search, for example, is usually conceptualized this way [@wolfePreattentiveObjectFiles1997; @nakayamaVisualSurfaceRepresentation1995], and a two-stage theory also appears to be implicitly assumed in two previous reviews of objects and tracking  [@schollObjectsAttentionState2001; @pylyshynSeeingVisualizingIt2006]. <!-- No interaction between number of objects to track and what kinds of objects can be tracked.--> It would be quite convenient if the assumption that object creation and object tracking occur at distinct processing stages were true, as that is more straightforward to study than an interactive system [@simonSciencesArtificialReissue1969; @sternbergDiscoveryProcessingStages1969].

```{r simpleArchitecture, echo=FALSE, out.width="55%", fig.cap="A schematic of the idea that objects are created prior to the action of tracking processes, which then point to the already-formed object representations but do not change them."}
knitr::include_graphics("imagesForRmd/flowDiagrams/objectCreationTrackingSeparateDiagram.png")
```

Certainly there is evidence that tracking operates on high-level representations. @maechlerAttentionalTrackingTakes2021 found evidence that tracking operates on perceived (illusory) object positions. Nevertheless, attention and object creation may be interactive. For example, the way stimulus elements are organized by attention can determine what illusory contours are created and perceived, as well as the lightness and depth that is perceived [@harrisonVoluntaryControlIllusory2019; @harrisonAttentionalSelectionIllusory2019; @peterVoluntaryAttentionModulates2005].  Our ability to perceive the complex motion of a human body from only several points of light highlights that object perception can involve hierarchical motion segmentation that reflects an interaction between Gestalt grouping and top-down knowledge of the overall shape of objects and the relative motion pattern of their parts [@johanssonVisualPerceptionBiological1973; @wangSearchingLifeMotion2010] (see the grouping Chapter (\@ref(grouping))). Further evidence for a role for neural feedback in object segmentation comes from @ongchocoHowCreateObjects2019, who asked participants to practice "imagining" a shape in a uniform grid of lines until they felt they could actually see the shape, which happened fairly readily. The detection of flashed probes was enhanced for those presented on the same imagined object, compared with equidistant ones presented on different objects. <!--In summary, a variety of , with some role for attention, but the extent of its importance remains unclear [@papaleInfluenceObjecthoodRepresentation2021; @wyatteEarlyRecurrentFeedback2014;-->
 
Potentially, the same attentional resources that mediate tracking may also contribute to the creation of object representations. One consequence would be a trade-off between the involvement of attention in constructing object representations and the number of objects that can be tracked. Informal experience with tracking the line ends in the @schollObjectsAttentionState2001 display seems to support this. If when you watch the movie of \@ref(fig:lines), you concern yourself with keeping track of the end of only *one* object, you are likely to succeed. But recall that it is difficult or impossible to accurately track *four* object ends - indeed, @schollObjectsAttentionState2001 found that participants' performance was approximately that predicted if they could track one line end, but not more. But this may not be due to the use of multiple-object tracking resources to create objects, but rather it may reflect System 2 processing that has a capacity of only one object.

The possible involvement of System 2 could mean that covert tracking of multiple objects is qualitatively different from covert tracking of a single object. Because the participants in the @ongchocoHowCreateObjects2019 study imagined only a single object, it is possible that their results reflect a capcity-one process rather than the processes we use to track multiple objects.
 
## What tracking sticks to

Even when all of our cognitive resources are brought to bear on a single entity, some entities still can't be tracked. @anstisEyesPursueMoving2010 showed this by asking participants to track the intersection of two shapes moving in a configuration that elicits the "chopsticks illusion". In that illusion [@anstisImperceptibleIntersectionsChopstick1990], a horizontal and vertical line slide over each other, with each line following a clockwise circular trajectory. Viewers perceive the intersection of the two lines to also be moving clockwise (see a demo [here](http://anstislab.ucsd.edu/illusions/chopsticks-illusion/)), but in fact the intersection moves counterclockwise only. This error may be, in part, a failure of object tracking, because if participants had been able to attentionally track the intersection, presumably they would have been able to judge its trajectory. @anstisImperceptibleIntersectionsChopstick1990 also found that participants could not accurately pursue the intersection with their eyes.

The true counterclockwise trajectory of the intersection becomes obvious perceptually if one views the display through a window so that the ends of the lines are occluded rather than visible, and in that condition participants were able to smooth pursue the intersection accurately. Evidently the illusion version of the display is interpreted in a way we cannot overcome, prior to the operation of tracking, even though what is to be tracked is a rather simply-defined point - an intersection. @anstisImperceptibleIntersectionsChopstick1990 suggested that the reason that the intersection is perceived to move in the wrong direction is because the clockwise motion of the ends of the lines is mistakenly assigned to the intersection, similar to how visibility of the ends of lines can veto the barber-pole illusion. As schematized by Figure \@ref(fig:simpleArchitecture), quite a lot of processing of motion and form occurs prior to the operation of tracking. <!--As we will see, however, there is also reason to believe that attention can mold some object representations.-->

```{r, echo=FALSE, out.width="100%", fig.cap="Some stimuli used by Howe et al. (2012). CC-BY"}
knitr::include_graphics("imagesForRmd/PiersHowe/PiersHoweStimuli.png")
```

As we saw in the "The ends of the line" section above, maintaining the representation of an undifferentiated part of an object is not something that our multiple object tracking processes are capable of. What sort of differentiation of object parts does allow tracking? This is not yet clear. @schollWhatVisualObject2001 and @howeCanAttentionBe2012 found seemingly-contradictory evidence for how distinct the ends of a dumbbell figure had to be from its center to allow tracking of the dubmbbell end. The could simply reflect the noisiness of the data of the two studies. @howeCanAttentionBe2012 also tested a "luminance" condition, pictured above, and found that performance (80% correct) was substantially lower than their baseline condition (96% correct), although not as low as for undifferentiated bar ends (72% correct). They were surprised that the clear difference in luminance between the targets and the connector in the luminance condition was not enough to keep tracking from being so adversely affected by the connectors.

Such results suggest that multiple object tracking uses a different segmentation of objects than what is available to us when we focus our attention on a single object. The findings resonate with results from visual search. @wolfePreattentiveObjectFiles1997 asked participants to search for conjunctions of features, such as red and vertical. If the vertical red part of an object were physically connected to a horizontal and green part, then participants were much slower to find the red vertical target segment in the display, among the green vertical and red horizontal distractors. In other words, it seemed that physically connecting one feature to another lumped it together as an undifferentiated collection of features from the perspective of search processes, what @wolfePreattentiveObjectFiles1997 termed a "preattentive object file". Unfotrtunately, no researcher seems to have tested displays of this nature for both tracking and search <!--FUTURE-->, so for now a parsimonious account is that multiple object tracking and search operate on the same object representations.

## Growth, shrinkage, and tracking

Some objects and substances change their shape as they move. A bathroom faucet, for example, will shoot a jet of water down into the sink, where the water flattens on the sink's bottom as it expands into a puddle. When beer is poured into a glass, a froth forms, which gradually thickens as the top of the liquid rises. These are examples of non-rigid, shape-shifting motion.

To find out what happens when people try to track non-rigid substances, @vanmarleAttentiveTrackingObjects2003 devised an object that moved a bit like an inchworm. In what I will refer to as the "slinky" condition, each object began as a square. The object then moved by extending its leading edge until it had the shape of a long and thin rectangle. Subsequently, the trailing edge of the slinky, which was still at its original location, would move forward until the slinky was a square again, now entirely at a new location. Performance was very poor in this task.
 <!--the substances condition difficulty seems to be accounted for by this problem-->

What causes the difficulty with slinkys? Knowing that should tell us something about how tracking works. @howeVisuallyTrackingLocalizing2013 tested a number of conditions that help to rule out various possibilities, such as the faster speed of the slinky's edges relative to the control conditions. The explanation for why slinkys are hard to track given by @schollWhatHaveWe2008 is "there was no unambiguous location for attention to select on this shrinking and growing extended object" because "each objectâ€™s location could no longer be characterized by a single point" (p.63). There may be something to this, but it is not entirely clear what is meant by an object's location not being characterizable by a single point. The objects more typically used for MOT, discs with no internal features, also have no unambiguous internal locations, because their insides are a completely undifferentiated mass. As a single point to represent their location, the objects' centroid could be used, but this seems just as true for an object changing in size and shape, like the slinky. A further problem for such an account is that in the chopsticks illusion reviewed above, the target was defined by a single point (the intersection of two lines), yet it could not be tracked.

An important clue is that both tracking and simple localization are disrupted by object expansion and contraction, which is one characteristic of slinky motion. After @howeVisuallyTrackingLocalizing2013 replicated the tracking findings of @schollWhatHaveWe2008, they probed the effect of size changes on localization. Participants were presented with a rectangle for 200 ms at a random location on the screen, and were asked to click on the location of the center of the rectangle. In a baseline condition, the rectangle did not change in size, shape, or location during its 200 ms presentation. In the size-change condition, the length of the object increased due to expansion for half of the interval and shrank due to contraction during the other half. Participants' localization errors were about 14% larger in this changing-size condition. This appeared to be driven by errors along the axis of the object's expansion and contraction, as errors in the orthogonal direction were not significantly different from the baseline condition. <!--FUTURE: Do a continuous mouse-tracking study of this to assess the lag and sources of noise with richer data. -->

The substantial localization impairment documented by @howeVisuallyTrackingLocalizing2013 may be the cause of the poor performance during MOT. However, that is still not clear. An important next step is to measure localization errors when the task is to monitor multiple objects changing in size rather than just one. If the localization deficit caused by change in size worsens with object load, this would help implicate the processes underlying tracking <!--FUTURE. Could do a continuous mouse-tracking version-->. This would not by itself answer, however, why exactly object localization is impaired by size changes.

## Could tracking work by attentional spreading?

The relationship between object representations and tracking schematized in Figure \@ref(fig:simpleArchitecture) suggests that attention selects *entire* object representation, however that object representation is created. I have not seen anyone suggest that how attention selects object representations might also facilitate tracking an object as it moves. However, this is quite possible.<!--INSIGHT-->

Consider that object selection may begin with selection of a particular location on that object, with attention subsequently spreading up to the edges of the object. Neurophysiological evidence for this has been observed in some tasks [e.g., @wannigAutomaticSpreadAttentional2011]. This spreading of attention may contribute to the ability to track moving objects. When an object moves, its leading edge will occupy new territory while its trailing edge continues to occupy an old location. If spreading of attention up to object boundaries continues to occur as an object moves, then attention should spread to the new locations near the leading edge. In such a fashion, attention could, by continually expanding to the new location of a leading edge and contracting with a trailing edge, stay on a moving object. Spreading of activation has been documented neurophysiologically in V1 of rhesus macaques who were given the task of evaluating whether two points are on the same curved line segment [@roelfsemaObjectbasedAttentionPrimary1998].

Presenting probes during a task of tracking multiple lines, @alvarezHowDoesAttention2005 found that probes presented at the center of objects were detected much more accurately than end probes, suggesting that attentional resources were concentrated on the centers of the lines. The spreading account seems to instead predict that accuracy would be highest near the trailing end of an object. It appears, however, that the researchers did not analyze the data to check whether of the two object ends, accuracy was higher for probes at the trailing end. Clearly, much more work <!--FUTURE apparent motion might also be a problem for this account, assuming you can track more than one object in apparent motion. Point objects, stepping objects (works for 1 target and probably for multiple ones if they are stepping a short distance, but I know of NO studies of MOT with apparent motion FUTURE), and the finding that attention tends to be ahead of an object? I don't recall whether that's been done --> is needed to reveal the nature of attentional spread while an object moves and any role that has in facilitating tracking.
<!-- Reminescent of how rapid eye movements (saccades) tend to go to the centroids of objects.-->

<!--Out the kitchen window, in the dense foliage of a tree, two birds squabble on the wing. As they plummet from one branch to another, you see only parts of each at any one time. An outstretched wing partially obscured by leaves and a branch, which folds in on itself as the bird alights on a branch and parts of its body comes into view. In addition to occlusion, camouflage can also cause the visible portions of animals to change in shape as they move from one background to another.-->

 <!-- Also use Zenon Pylyshyn's examples of objects not represented well as objects, so PylyshynAttention_Lecture_class -->

<!--Many questions remain regarding what sorts of objects attention gloms onto.


