# The biggest myth {#biggestMyth}

It is often stated in the literature that people can only track four or five objects. @doranRoleVisualAttention2010, for example, claim that "the main finding" from the object tracking literature "is that observers can accurately track approximately four objects and that once this limit is exceeded, accuracy declines precipitously." Similarly, writing about the "object tracking system", @piazzaNeurocognitiveStartupTools2010 wrote that "One of the defining properties of this system is that it is limited in capacity to three to four individuals at a time".

This biggest myth about tracking is actually two claims. The first is that there is a constant limit of around four objects. The second is that, regardless of whether the limit varies across circumstances, accuracy declines precipitously [@doranRoleVisualAttention2010] when the limit is reached.

I have examined the references cited by several papers that might reasonably be taken to be claiming a version of either the first or second claims. Unfortunately, typically authors do not distinguish between these two claims, instead they make a vague statement that might be taken to refer to either or to both. Accordingly, vague statements had to be included in my search, in particular the following published statements:
"People’s ability to attentively track a number of randomly moving objects among like distractors is limited to four or five items"  <!--cites Pylyshyn Storm 1989--> [@fougnieDistinctCapacityLimits2006],  <!-- which cites Scholl's objects paper which makes the claim of 4 FINSTs but backs it up only with STM etc. type studies -->, "researchers have consistently found that approximately 4 objects can be tracked"  @alvarezHowManyObjects2007, "people typically can track four or five items" @chesneyEvidenceSharedMechanism2011 <!--cites Pylyshyn 1989-->, "participants can track about four objects simultaneously" @vanderburgChangesNotDifferences2019 <!-- (see Cavanagh & Alvarez, 2005, for a review; Pylyshyn & Storm, 1988)-->. In each of these cases, I have checked the evidence provided, or the papers cited, as well as the papers those papers cite. Each paper  provides no evidence supporting the claim that performance decreases very rapidly once the number of targets is increased above some value. Those that do have evidence that is relevant find only a gradual decrease in performance as number of targets is increased with no discontinuity, nor even a conspicuous inflection point. For example, @oksamaMultipleObjectTracking2004 designated between two and six objects as targets, among twelve identical objects in total. After the objects moved around randomly for five seconds, one object was flashed repeatedly and participants hit a key to indicate whether they thought it was one of the targets. The proportion of trials in which participants were wrong increased steadily with target size, from 3% incorrect with two targets, to 16% incorrect with six targets. Note that even with six targets, participants were performing substantially better than would be expected if they could only track one or two and had to guess on the others.

Even @pylyshynTrackingMultipleIndependent1988, the paper most frequently invoked when a limit of four objects is claimed, found a quite gradual decrease in performance (their Figure 1) as the number of targets was increased from one to five, and five was the most targets that they tested. In their paper, @pylyshynTrackingMultipleIndependent1988 do not suggest that there is a hard or precipitous limit, but in 1994, @pylyshynMultipleParallelAccess1994 did write that it is "possible to track about four randomly moving objects", even though earlier in that paper they contradict this somewhat by writing "at least four". I suspect that some cases of this sort of slide toward backing a consistent and hard limit reflects a desire for a simple story. It may also stem from an unconscious oversimplification of one's own data, as well as the theoretical framing by @pylyshynTrackingMultipleIndependent1988 that tracking was limited by a set of discrete mental pointers.

Recall that the myth is associated with two claims, not just that there is a "limit" after which performance decreases rapidly, but also that this limit is consistently found to be about four. Because the published evidence indicates that the first claim is incorrect, let's put that aside and consider the claim that tracking performance falls, even if not rapidly, to some particular level, such as 75% correct, at about four targets. Or instead of a percent correct criterion, alternatively the criterion could be something like the halfway point from ceiling to chance performance, or the "effective number of items tracked" [@schollWhatVisualObject2001] calculated by applying a formula to percent correct together with the number of targets and distractors.

Several early studies can perhaps reasonably be described as finding that a similar number of objects can be tracked before accuracy falls by much. <!-- Alvarez & Franconeri (2007)  "researchers have consistently found that approximately 4 objects can be tracked (Intriligator & Cavanagh, 2001; Pylyshyn & Storm, 1988; Yantis, 1992). The similarity of these estimates, combined with the frequency with which 4-item limits arise in other attention tasks, suggests the possibility that there is a “magical number 4” in visual attention (Cowan, 2001; Pylyshyn, 1989)."  neither said any such thing, it seems from my reading of those papers, e.g.  Pylyshyn & Storm 1988 only went up to 5 out of 10 and there was gradual decrease in performance all the way through. --> <!-- Papers I have checked for the 4-object claim: Cowan, 2001 claimed it for MOT, citing Pylyshyn et al. 1994, which does say "experiments showing that observers can simultaneously track some three to five identical target items" but then when he describes the studies he doesn't say they show that, even though in the discussion he says "One is that it is possible to track about four randomly moving objects" -->
<!-- In some cases the problem stems from Scholl Pylyshyn Feldman paper that devised the effective tracking capacity measure assuming 100% accuracy for one object and delivering a single number, often 4, for effective number tracked--> However, work published during this century revealed this to be an artifact of researchers using similar display and task characteristics. One of the most salient of these characteristics is object speed.

@alvarezHowManyObjects2007 used a display of sixteen circles that wandered about the screen, and found that the fewer in number were the targets to track, the faster participants could track them. Conversely, at very high speeds, participants could only track one object with reasonable accuracy. The finding that participants can track at least four objects with reasonable accuracy is restricted to particular object speeds was also found by others [@holcombeExhaustingAttentionalTracking2012] and it was also found that the number that can be tracked depends on the objects' spacing [@franconeriEvidenceSpeedLimit2008; @holcombeObjectTrackingAbsence2014]. As we will be discussed extensively in section \@ref(twoBrains), it also can depend greatly on whether the targets are distributed between the left and right hemifields or instead are confined to one hemifield.

In summary, it is wrong to say that people can track about four moving objects or that once some number of targets is reached, performance declines very rapidly. It is also wrong to say that people can track "about four objects". The number that can be tracked is quite specific to the display arrangement, object spacing, and object speeds. Nevertheless, something like the limit-of-four statement can be made true if it stipulated certain tasks, display characteristics, and performance measures.

In 2013, a diverse set of two dozen working memory researchers attended a workshop with the purpose of "developing benchmarks for models of working memory". They grappled with, among other issues, how to characterize the limit on how many items people can remember. In their paper that ultimately resulted from the workshop, the researchers pointed out that "observed item limits vary substantially between materials and testing procedures" [@oberauerBenchmarksModelsShortterm2018]. Nevertheless, they suggested that much of this variability could be explained by humans' ability to store groups of items as "chunks" and thus they endorsed a statement that there is a limit of "three to four chunks" [@cowanMagicalNumberShortterm2001a]. Hence, these researchers believed they could explain the observed variability in experiments' results by a common underlying limit of three to four chunks that manifests as different observed item limits depending on circumstances, in particular opportunity for chunking.

In the case of MOT, possibly researchers can identify a set of circumstances that consistently yield a mean tracking limit of three or four targets (if "limit" is  defined as performance falling to a particular level on some performance metric). Perhaps these circumstances will simply be certain spacings, speeds, object trajectories, and number of objects in a display. Ideally, however, some underlying construct (the counterpart of chunks for memory) would be identified to explain how performance changes when other circumstances are used. That would constitute real progress in theory development.

<!-- At this point,  to defend the claim that people can track "about four objects" is to suggest that the studies that have found four targets to be the number that can be tracked . Is there something special about  these speeds. Nobodh justified them with ecological analysis-->

## Different tasks, same limit?

While it is not correct to say that people can track about four objects, there is an important related claim that has more support. This is frequently wrapped up in the myth, for example phrased as the idea of a "magical number four", but referring to the notion that very different tasks have the same limit. For example, @bettencourtSharedFilteringProcesses2011 write that "both processes [visual short-term memory and MOT] showing an equivalent four-object limit", and @piazzaNeurocognitiveStartupTools2010 similarly claimed that visuo-spatial short-term memory, ultra-rapid counting (subitizing), and multiple object tracking all share a limit of "three or four items".

The literature on subitizing, sometimes known as numerosity perception, has a very long history of claiming that accuracy in counting the objects in a brief exposure has a real limit of about four objects - in the sense of a sudden decrease in accuracy when the number of objects shown goes from less than four to more than four [@jevonsPowerNumericalDiscrimination1871; @revkinDoesSubitizingReflect2008]. In the "subitizing range" of four or fewer objects, performance is approximately as good for rapidly counting four objects as it is for two or one. It seems this proposition is well-supported; while some have questioned it [@dehaeneDevelopmentElementaryNumerical1993], additional evidence has accumulated for a truly distinct subitizing range [@revkinDoesSubitizingReflect2008a]. It has often been also claimed that there is a limit of about four objects that can be stored in visual working memory [e.g., @cowanMagicalMysteryFour2010]. Whether there is a discontinuity after four objects or at any point remains highly debated, however [e.g. @robinsonThereCapacityAssessing2020].

At the level of a common limit in terms of number, then, it remains unclear whether tasks such as object tracking, visual working memory, and subitizing can be said to have a common limit. Ideally this could be confirmed by measuring the limits for all three tasks using the same stimuli, but it is unclear how to equate difficulty across tasks. Especially difficult is comparing performance with the briefly-presented static stimuli used in subitizing and working memory tasks to the extended exposures of moving stimuli needed to assess object tracking. A modeling approach could make progress on this issue, although it would require making assumptions that might need novel empirical support. Another approach is to determine which tasks' limits co-vary between individuals. This is reviewed in section \@ref(abilities).

<!-- https://psyc2016.whatanimalssee.com/bottlenecks.html#a-bottleneck-for-object-judgments -->

<!--Working off the dominant framing, that each person does have a specific number of targets they can track which determines percent correct for each level of number of targets, some MOT researchers report what @schollWhatVisualObject2001 called the "effective number of items tracked". The associated formula, refined by @hullemanMathematicsMultipleObject2005a allows researchers to calculate the effective number of items tracked based on accuracy in an individual condition, given the number of targets and distractors in that condition. This does provide a useful summary of the data, but researchers should take more care to avoid taking it literally.--> <!--Still, it does seem that the number four pops up more than would be expected if these abilities were unrelated. -->

To summarize this section, there are three common misconceptions about what has been shown about object tracking. The one that we have just discussed is that different tasks show the same limit. This may well be true, but I know of no research that have used the same display and task settings to adequately back this up. Now returning to the previous section, it is not justified to say that as the number of targets is increased to about four, performance falls to a certain criterion level. One must specify particular display and task characteristics in order to make that statement true. A final misconception is that performance falls very rapidly when one increases the number of targets past a particular number (the "limit").

Given that tracking performance does depend greatly on circumstances and falls gradually rather than displaying a discontinuity at a particular target number, what are the implications for how tracking works? Briefly, these characteristics of tracking are particularly consistent with resource theories, which are discussed in section \@ref(whichAspects).
