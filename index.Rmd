--- 
title: "Attending to moving objects"
author: "Alex O. Holcombe"
date: "`r paste0('Updated on ',Sys.Date())`"
site: bookdown::bookdown_site
# The weird thing is that including only the gitbook command below is enough to create the PDF and ePub as well as the html 
output: 
  #gitbook renders html
  bookdown::gitbook:
      lib_dir: assets
  #pdfbook includes images but not DiagrammeR output, like for peripheral vision demos
  bookdown::pdf_book:
    includes:
      in_header: preamble.tex
    latex_engine: xelatex
    citation_package: natbib
    number_sections: FALSE
    keep_tex: yes
    toc_depth: 1
    lof: yes
  bookdown::epub_book: default
  #bookdown::word_document2
documentclass: book
always_allow_html: true
bibliography: [bibliography/CambridgeElementNewestAdditions.bib,  bibliography/packages.bib]  
#bibliography/CambridgeElement.bib, 
biblio-style: "apalike"
#csl: apa7-single-spaced.csl
link-citations: yes
url: 'https\://tracking.whatanimalssee.com/'
github-repo: alexholcombe/tracking-review
twitter-handle: ceptional
#For the order of chapters, see _bookdown.yml, where they are manually specified
description: "This is an opinionated book that reviews some of the literature on multiple object tracking by humans."
#cover-image: "imagesForRmd/threeWiseMonkeys/Three_Wise_Monkeys_640px.jpeg"
---
<!--other titles: Tracking moving objects, How humans track objects, Attending to a moving world -->

# Preface {-}

Cite this as:

Holcombe, A.O. (to appear). Attending to moving objects. Cambridge University Press.

This book, posted with chapter hyperlinks, cross-references, and embedded movies at [tracking.whatanimalssee.com](tracking.whatanimalssee.com),
reviews some of what we know about multiple object tracking by humans. A revised version is expected to be published by Cambridge University Press in their [Cambridge Element series](https://www.cambridge.org/core/what-we-publish/elements/elements-in-perception).
<!--https://mc.manuscriptcentral.com/hsselements1?URL_MASK=8f1bd56d44104a8b82eafbb42c3bdb84-->

An earlier draft that you may have found here greatly exceeded the publisher's word limit, so I have moved material on temporal limits and on serial versus parallel processing into [a separate manuscript](trackinglimits.whatanimalssee.com). 

<!--You can read this [here on the web](https://tracking.whatanimalssee.com/index.html), as a [PDF file](bookdown-demo.pdf), or as an [e-book](bookdown-demo.epub), which you can import into your Kindle or other e-book reader.--> 

Contact me (he/him) with any comments via [twitter](https://twitter.com/ceptional) or email - alex.holcombe@sydney.edu.au

```{r AlexPhoto, echo=FALSE, out.width="25%"}
knitr::include_graphics("imagesForRmd/corellaOnShoulder2020croppedBlurredByAdobeOnline.jpg")
```

I thank Hrag Pailian, Brian Scholl, Lorella Battelli, and the three anonymous reviewers for helpful comments. I also thank Hrag Pailian and Brian Scholl for providing images and movies of their work.

© Alex O. Holcombe 2022

## Abstract

Our minds show extraordinary limitations on how much visual information they can extensively process, in spite of massive parallel processing at our visual front ends. This is especially evident when participants attempt to track  moving objects. Humans' capacity for updating object representations is more limited than indicated by studies with static stimuli. Unlike many abilities, our ability to track multiple objects is mediated by processes specific to each cerebral hemisphere, such that each hemifield has its own capacity limit. However, tracking can also benefit from more cognitive processes not specific to each hemifield. Here, recommendations are made for improvimg methodology so that the relative role of these two kinds of processes can be distinguished. We are far from fully understanding how attention keeps up with multiple moving objects, but what we already know is fascinating and there are promising directions for new discoveries.

<!--Reviewers: Hyona/Pailian/Saiki for identity chapter; Piers Howe / John Palmer/Daniel Little for serial/parallel; Srimant Tripathy for sensory memory ; Scott Brown at Newcastle for drift diffusion models
-->

<!-- To launch CANVAS VIDEOS, I tried looking at the private hyperlink within the Canvas page and used
https://www.url-encode-decode.com to unescape the characters, but that yields failed launch:

[canvas video](https://sydney.instructuremedia.com/lti/launch?custom_arc_display_download=true&custom_arc_launch_type=embed&custom_arc_media_id=83d5fa8f-2601-4cde-8500-16b22da451f4-79254)

This doesn't work either:
[Canvas video](https://sydney.instructuremedia.com/embed/83d5fa8f-2601-4cde-8500-16b22da451f4-79254)
-->
<!-- CANVAS quizzes - I can't see any way to make external links work, even for a Canvas Commons quiz, it takes you to a bizarro Canvas login https://lor.instructure.com/resources/76da8b14c91a40d885c6fe0452bf33f4?shared -->

<!--Wordcount
From R environment, execute the following after eliminating the references
wordcountaddin::word_count('tracking-review.Rmd')
Approximately 27,500 words without references
-->

<!--Exporting to Word: Looked less fucked-up if I first imported the PDF into Google Docs and then clicked Open in->Microsoft Word.app-->

<!-- Speed limits and temporal limits graph fails in PDF version. Is that because of “Warning: LaTeX Warning: Float too large for page by 59.24739pt on input line 584.”?  I think I fixed it by making it smaller.

In PDF, plot of results fails to render the special plot symbols.--> 

<!--
```{theorem, name="Pythagorean"}
For a right triangle, if $c$ denotes the length of the hypotenuse
```
-->
