# Spatial interference {#spatialInterference}

Details of the world that are much smaller than ourselves, like the fibers of a piece of paper, or the individual blotches laid down on paper by a printer, are inaccessible to the naked eye, largely because our photoreceptors are too widely-spaced. This is a familiar limit on our visual abilities, one that is measured every time we go to the optometrist. Line segments or objects that are too close together are experienced is a single unit.

Critically, even when two objects are spaced far apart enough that they can easily be seen to be two objects rather than one, they are not processed entirely separately by the brain. Receptive fields grow larger and larger as visual signals ascend the visual hierarchy, and this can result in a degraded representation in the visual system for objects that are near each other. In other words, spatial interference ensues.

<!--Decades before @franconeriHowManyLocations2007 conducted experiments on the selection of multiple locations with different densities, researchers had recognized the existence of spatial interference in dense displays-->
A large body of psychophysical work has investigated the display densities that impair object perception. Common tasks in this literature include letter identification and grating orientation discrimination [e.g., @wolfordPerturbationModelLetter1975; @korteUberGestaltauffassungIm1923; @strasburgerDancingLettersTicks2014].

```{r, echo=FALSE, fig.cap = "When one gazes at the central dot, the central letter to the left is not crowded, but the central letter to the right is."}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = LR]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle, style = filled, color=White, fillcolor = White, fontsize = 40]

a [label = 'O']
fixation [label =  '', shape=circle, fillcolor=Black, width=.2, height=.2]
b [label = ' ']
c [label = ' ']
d [label = 'J']
e [label = ' ']
e2 [label = ' ']
e3 [label = 'S']

f [label = 'O']
g [label = 'R']
h [label = 'L']
i [label = 'H']
j [label = 'Y']
k [label = 'M']
l [label = 'S']

# edge definitions with the node IDs
edge [label='', penwidth=0, arrowsize=0]
a  -> b;
edge [label='', penwidth=0, arrowsize=0]
b -> c
edge [label='', penwidth=0, arrowsize=0]
c -> d
edge [label='', penwidth=0, arrowsize=0]
d -> e
edge [label='', penwidth=0, arrowsize=0]
e -> e2
edge [label='', penwidth=0, arrowsize=0]
e2 -> e3
edge [label='', penwidth=0, arrowsize=0]
e3 -> fixation

edge [label='', penwidth=0, arrowsize=0]
fixation -> f
edge [label='', penwidth=0, arrowsize=0]
f -> g
edge [label='', penwidth=0, arrowsize=0]
g -> h
edge [label='', penwidth=0, arrowsize=0]
h -> i
edge [label='', penwidth=0, arrowsize=0]
i -> j
edge [label='', penwidth=0, arrowsize=0]
j -> k
edge [label='', penwidth=0, arrowsize=0]
k -> l
}")
```

In the above display, if you gaze at the central dot, you likely will be able to perceive the middle letter to the left fairly easily as a 'J'. However, if while keeping your eyes fixed on the central dot you instead try to perceive the central letter to the right, the task is much more difficult. This spatial interference phenomenon is called "crowding" in the perception literature.

Most studies of crowding ask participants to identify a target such as a letter when flanking stimuli are placed at various separations from the target. The separation needed to avoid crowding varies depending on the display spatial arrangement, but on average is about half the eccentricity of the target; the interference diminishes rapidly as separation increases beyond that [@boumaInteractionEffectsParafoveal1970a; @gurnseyCrowdingSizeEccentricity2011]. In the display above, for example, the letters on the same side as the 'J' are separated from it by more than half the 'J's distance from the fixation point, so they have little to no effect on its identification.

When flankers are presented close to a target, they not only prevent identification of the target, they can also prevent the target from being individually selected by attention, which likely spells trouble for multiple object tracking [@intriligatorSpatialResolutionVisual2001]. When a target and distractor are too close to be distinguished by tracking processes, the tracking process may inadvertently end up tracking a distractor rather than the target.

Crowding happens frequently in typical MOT displays, in that in most experiments, objects are not prevented from coming within half the eccentricity of each other. It is not surprising, then, that in typical MOT displays,
greater proximity of targets and distractors is associated with poor performance [@shimSpatialSeparationTargets2008; @tombuAttentionalCostsMultipleobject2008].

## Spatial interference does not explain why tracking many targets is more difficult than tracking only a few

<!-- MOVE TO PREFACE TO SPATIAL AND TEMPORAL INTERFERENCE The errors in MOT occur largely when a target and a distractor come close to each other, in space or in time. During a close encounter between a target and a distractor, one may end up tracking the distractor rather than the target.
 @baeCloseEncountersDistracting2012 @drewSwappingDroppingElectrophysiological2012 -->

@franconeriTrackingMultipleObjects2010a claimed that spatial interference is the *only* reason why performance is worse when more targets are to be tracked, denying any role for speed, time, or a resource that is depleted when more targets must be tracked. They posited that tracking errors, other than lapses of concentration, are caused entirely by the poor spatial resolution of tracking's selection process, together with inhibitory surrounds that can cause mutual interference among the selection of nearby targets. The most provocative tenet of their theory was that "there is no limit on the number of trackers, and no limit per se on tracking capacity" (p.920), implying that a very large number of targets could be tracked if they were kept far apart from each other.

In pursuing tests of this theory, @franconeriTrackingMultipleObjects2010a unfortunately did not isolate the separation between objects from other display variables. More generally in the literature, MOT studies rarely control the retinal separation among the objects in a display. For example, @franconeriTrackingMultipleObjects2010a kept object trajectories essentially constant in their experiments but varied the total distance traveled by the objects (by varying both speed and trial length), on the basis that if close encounters were the only cause of errors, they should be proportional to the total distance traveled. When they found that performance did decrease with distance traveled, but there was little effect of the different object speeds and trial durations that they used, they took this as strong support for their theory that only spatial proximity mattered.

As @franconeriTrackingMultipleObjects2010a wrote in their conclusion, their hypothesis that "barring object-spacing constraints, people could reliably track an unlimited number of objects as fast as they could track a single object" constituted a "simple and falsifiable hypothesis". I believed this hypothesis to be unlikely to be true, and on obvious test was to keep all the objects in an MOT display widely separated, vary the number of targets, and measure the speed threshold in each case.

In 2012, my student Wei-Ying Chen and I conducted several experiments in this vein. In one, we used an ordinary computer screen but had participants bring their noses quite close to it to create a wide-field display, which allowed us to keep targets and distractors dozens of degrees of visual angle from each other @holcombeExhaustingAttentionalTracking2012. The basic display configuration is shown below <!-- Figure \@ref(fig:HC2012BasicTrial) CAUSES LATEX ERROR-->.

```{r HC2012BasicTrial, echo=FALSE, out.width="100%", fig.cap="In experiments by Holcombe & Chen (2012), after the targets were highlighted in white, all the discs became red and revolved about the fixation point. During this interval, each pair of discs occasionally reversed their direction. After 3–3.8 s the discs stop, one ring is indicated by presenting text next to it, and the participant clicks on one disc of that ring."}
knitr::include_graphics("imagesForRmd/HolcombeChen2012BasicTrial.png")
```

We found that even with objects extremely widely-spaced speed thresholds declined dramatically with the number of targets. To us, this appeared to falsify the theory of @franconeriTrackingMultipleObjects2010a. Indeed, for the 2011 Vision Sciences Society conference where we reported these findings, we entitled our poster entitled "The resource theory of tracking is right! - at high speeds one may only be able to track a single target (even if no crowding occurs". What we meant was that each target takes up some of a very limited processing capacity - a resource that was attentional in that it could be applied anywhere in the visual field, or at least anywhere within a hemifield (\@ref(twoBrains)). The amount of this resource applied to a target determines the fastest speed at which a target can be tracked.

Franconeri, together with his colleagues George Alvarez and Patrick Cavanagh (of whom the latter was, incidentally, my PhD advisor) were not convinced by the findings of Wei-Ying Chen and I. Franconeri et al. not only continued to push their spatial interference theory of tracking, but also they suggested the same basic idea could explain capacity limits on object recognition, visual working memory, and motor control, writing that "competitive interactions are the roots of capacity limits for tasks such as object recognition and multiple object tracking” (p.2) and that capacity limits arise only because "items interact destructively when they are close enough for their activity profiles to overlap" (p.2) @franconeriFlexibleCognitiveResources2013. Yet to explain the @holcombeExhaustingAttentionalTracking2012 results, the spatial interference posited by Franconeri would have to extend over a very long distance, farther than anything that had been reported in behavioral studies. Furthermore, if there were such long-range spatial gradients of interference present, they should have shown up in the results of @holcombeExhaustingAttentionalTracking2012 as worse performance for the intermediate spatial separations we tested than for the largest separations we tested.

To address these issues with their theory, @franconeriResourceTheoryNot2013 appealed to data from recordings in the lateral intraparietal area (LIP) of rhesus macaque monkeys. In a study by @falknerSurroundSuppressionSharpens2010, the monkeys were presented with a saccade target, and the monkey was cued to execute a saccade to it by offset of the fixation point. In some trials, however, another stimulus was flashed 50 ms prior to the offset of the fixation point. That flashed stimulus was positioned in the receptive field of an LIP cell the researchers were recorded from, and it was found that its response was suppressed relative to trials when there was not a saccade target. This was true even when the saccade target was very far away, with statistically significant impairment at separations up to 40 deg for some cells. There was a spatial gradient, but the data suggested it could be quite shallow, allowing @franconeriResourceTheoryNot2013 to write that "levels of surround suppression are strong at both distances, and thus no difference in performance is expected" for the separations tested by Holcombe and Chen (2012). Note that @franconeriResourceTheoryNot2013 was published as an online comment at _Trends in Cognitive Sciences_, as a reply to my online comment pointing out some of the problems with their theory. Unfortunately, some time later, probably in 2018, both comments were lost by the publisher, Elsevier, when they migrated their system. In the case of my comment, however, I found an old version on my computer, updated it slightly and posted it at @holcombeCommentCapacityLimits2019.

One property of the neural suppression documented by @falknerSurroundSuppressionSharpens2010 strongly suggests it is not one of the processes that limit our ability to track multiple objects. Specifically, @falknerSurroundSuppressionSharpens2010 found that nearly as often as not, the location in the visual field that yielded the most suppression was not in the same hemifield as the receptive field center. But as we will see in \@ref(twoBrains) the cost of additional targets in attentional tracking is largely independent in the two hemifields, suggesting that LIP suppression is not the main factor yielding worse performance when there are more targets. Instead, as @falknerSurroundSuppressionSharpens2010 themselves concluded, these LIP cells may help mediate a global salience computation for prioritizing saccade or attentional targets wherever they are in the visual field.

In light of all the above, it seemed the evidence ruled against the idea that spatial interference was the sole reason that people perform worse with more targets.  By 2013, the spatial interference account advocated by Franconeri seemed to have been watered down until it was practically indistinguishable from a resource theory. That is, if spatial interference extended over an entire visual field (or hemifield) with no detectable diminution at large separations relative to small separations, then it no longer seemed appropriate to refer to it as "spatial" interference. Instead, finite processing capacity seems to be both a more parsimonious and straightforward description.

The word "resource" appropriately conveys that people can choose how to apply their finite processing capacity, as a resource is something that is available to someone and can be used in different ways. For example, it suggests that one can use half of one's processing capacity for one target while using the other half for a second target. The validity of this part of the metaphor is addressed by some experimental evidence reviewed in \@ref(twoBrains) and \@ref(serialOrParallel).

Having failed to find evidence for long-range spatial interference, I decided to investigate the form of spatial interference that I was confident actually existed: short-range interference. As mentioned in the first section of this chapter, we were already confident that short-range spatial interference existed, from some previous studies of tracking. However, these studies did not provide much evidence about how far that interference extended - either they did not control eccentricity (e.g. @feriaSpeedHasEffect2013) or they only tested a few separations (e.g. @tombuTrackingPlanetsMoons2011). 

In experiments published in 2014, we assessed tracking performance for two targets for various separations between the targets' trajectories [@holcombeObjectTrackingAbsence2014]. Performance improved with separation, but only up to a distance of about half the target's eccentricity, similar to what has typically been found in the crowding literature [@strasburgerDancingLettersTicks2014]. In a few experiments there was a trend for better performance even as separation increased beyond the crowding zone, but this effect was small and not statistically significant. In all experiments performance was substantially worse for large separations, even for the largest separation tested of 59 degrees of visual angle. These findings supported my suspicion that spatial interference is largely confined to the crowding range, and that the performance deficit when there are more targets to track is caused by a limited processing resource.

The experiments we reported in @holcombeObjectTrackingAbsence2014 did yield one surprise. In the one-target conditions, outside the crowding range we found that performance actually decreased with separation from the other pair of (untracked) objects. This unexpected  cost of separation was only statistically significant in one experiment, but the trend was present in all four experiments that varied separation outside the crowding range. This might potentially be explained by the configural or group-based processing that I reviewed in \@ref(grouping), as grouping of distant elements tends to occur less often than for nearby elements [e.g., @kubovyLawfulnessGroupingProximity1998].

What is the nature of the short-range spatial interference that we and others have documented? As explained in the beginning of this chapter, one cause of spatial interference is simply lack of spatial resolution by the processes that mediate tracking. If a process cannot distinguish between two locations, either because of a noisy representation of those locations or because the two locations are treated as one, then a target may often be lost when it comes too close to a distractor. This would be true of any imperfect mechanism, regardless of whether it is biological or man-made.

While some form of spatial interference or confusion is well-nigh universal, the particular way that the human visual system is put together may result in forms of spatial interference that do not occur in, for example, computer algorithms engineered for object tracking. As described in \@ref(bottlenecks), our visual processing architecture has a pyramid-like structure, with processing at the retina being local and massively parallel, and then gradually converging such that neurons at higher stages have receptive fields responsive to large regions. At these higher stages processes critical to tasks like tracking or face recognition occur. Face-selective neurons, for example, are situated in temporal cortex and have very large receptive fields. For tracking, while the parietal cortex is thought to be more important than the temporal cortex, the neurons in these parietal areas again have large receptive fields.

A large receptive field presents a problem when the task is to recognize an object in clutter or to track a moving target in the presence of nearby moving distractors. In the case of object recognition, without a mechanism to prevent processing of the other objects that share the receptive field, object recognition would have access to only a mishmash of the objects' features. Indeed, this indiscriminate combining of features is thought to be one reason for perceptual illusory conjunctions. Similarly, for object tracking, isolating the target is necessary to keep it distinguished from the distractors.

In principle, our visual systems might include attentional processes that when selecting a target can completely exclude distractors' visual signals from reaching the larger receptive fields. Actually implementing such a system using realistic biological mechanisms with our pyramid architecture, however, looks to be difficult [@tsotsosModelingVisualAttention1995]. Neural recordings reveal that while the signals from stimuli irrelevant to the current task are suppressed somewhat, they still affect neural responses. The particular mechanism used by our visual system appears to include active suppression of a region around a target, which has long been championed by the computer scientist John Tsotsos as a practical way for high-level areas of the brain to isolate a stimulus in their large receptive fields. This suppression likely involves descending connections from high-level areas and possibly recurrent processing [@tsotsosDifferentStagesVisual2008].

Note that on this account, it is only targets, not distractors, that have a region of suppression surrounding them. One consequence of this is that when one tracks more than one target, there is the potential for the targets to suppress each other's selection process. Indeed, this was one basis of the contention of @franconeriTrackingMultipleObjects2010a that the limit on number of targets that can be tracked was caused entirely by spatial interference. While their attempt to attribute all of the cost of tracking additional targets to surround suppression may be misguided, in @holcombeObjectTrackingAbsence2014 we did find some tentative evidence supporting a greater range of interference in the two-target condition compared to the one-target condition. Again, the effect was small relative to the total additional-target performance cost. It appears that overlapping surround suppression associated with targets may impair tracking, but the spatial range of this does not extend much beyond the classic crowding range.

<!-- p.11:"the effect of separation was not significantly greater in the two- target condition than the one-target condition, but the difference did approach significance. Experiment 2: F(1, 7) 1⁄4 3.52, p=0.103. Experiment 3: F(1, 9) 1⁄4 3.89, p=0.054. Such an interaction would be consistent with the proposal that attending to an object results in an inhibitory surround, as attention to the two targets could then inhibit each other. This interaction is small, however, relative to the size of the additional-target cost (see Figure 7), suggesting that crowding is not responsible for much of the additional-target cost."
--> 

<!--While the @franconeriTrackingMultipleObjects2010a spatial interference theory is still frequently cited uncritically, the evidence against it seems to be strong.-->

Crowding, or the impairment of object identification by nearby objects, has generally been studied more extensively than the spatial interference associated with object tracking. Like for tracking, however, the possibility of suppression around targets remains understudied. Very few studies of crowding have varied the number of targets. I have found one study of the identification of briefly-presented stimuli which found that attending to additional gratings within the crowding range of a first grating resulted in greater impairment in the letter identification task [@mareschalAttentionalModulationCrowding2010]. This is consistent with the existence of surround suppression around each target. <!--Unfortunately, however, the study did not investigate how much further, if at all, spatial interference extended when there are more targets.--> 

<!--As reviewed by @holcombeObjectTrackingAbsence2014, while dozens of MOT papers have manipulated spatial proximity, few have both required fixation and scaled separation with eccentricity such that the relationship of target load and the range of spatial interference could be directly assessed.-->

<!--@maki-marttunenDistinctNeuralMechanisms2019:"In both cohorts, increased load and close encounters (i.e., close spatial proximity) led to reduced accuracy in an additive manner. Load was associated with pupil dilations, whereas close encounters were not. Activity in dorsal attentional areas and frequency of saccades were proportionally larger both with higher levels of load and close encounters. Close encounters recruited additionally ventral attentional areas that may reflect orienting mechanisms. The activity in two brainstem nuclei, ventral tegmental area/substantia nigra and locus coeruleus, showed clearly dissociated patterns. Our results constitute convergent evidence indicating that different mechanisms underlie processing challenges due to load and object spacing."-->

<!--Distractors that pass closer to targets can experience more inhibition (as measured by probes on objects; @doranRoleVisualAttention2010)-->

<!--MENTION THAT HOLCOMBECHEN FOUND NO EVIDENCE FOR SPATIAL INTERFERENCE WITH 12 OBJECTS SHARING A CIRCULAR TRAJECTORY-->

In summary, spatial interference likely contributes to many errors in tracking when targets and distractors come close to each other. This may be mediated partly by surround suppression around targets, as well as the inherent ambiguity regarding which is a target and which is a distractor during close encounters for a limited spatial resolution system. When objects are kept widely separated, it appears that spatial interference plays little to no role in tracking. Some other factor or factors, such as some sort of attentional resource, is needed to explain the dramatic decline in tracking performance that can be found with more targets even in widely-spaced displays [@holcombeObjectTrackingAbsence2014; @holcombeExhaustingAttentionalTracking2012; @holcombeSplittingAttentionReduces2013].
<!--As will become clear in the next section, however, in such conditions, temporal interference can determine tracking performance. -->

<!--
## Spatial selection of multiple locations

While multiple object tracking seems to require maintaining selection of moving objects, one can also ask about the capacity to maintain selection of stationary objects. If one cannot select the objects when they are stationary, perhaps one has no chance of tracking them when they are moving. Actually, the differing motion direction of moving objects may facilitate distinguishing among them, which will be discussed in section \@ref(beyondLocation), but nevertheless the processes that allow selection of multiple stationary objects are very likely part and parcel of those that support tracking.

Is selection of static objects resource-limited? @franconeriHowManyLocations2007 investigated this with two concentric circular arrays of stationary dots that were centered on fixation. Between one and eight of the dots were briefly highlighted, and then each dot was replaced by either a small horizontal or a small vertical bar. The participants' task was to search for a vertical bar, which was guaranteed to appear in the previously-cued locations. The participants were to press one key if a vertical bar was present among the cued locations, and another key if none of those locations contained a vertical bar.

In a sparse display with twelve locations, @franconeriHowManyLocations2007 found that average performance dropped from 98% when two locations were cued to 91% when six locations were cued. This decrease is fairly small, suggesting that if the result were to generalize to typical MOT displays, spatial selection processes would contribute only a small portion of the performance decrease with greater set sizes. However, MOT studies frequently allow objects to come much closer to each other than the spacing that @franconeriHowManyLocations2007 used in their sparse condition. In the denser conditions tested by @franconeriHowManyLocations2007, performance again started at a very high level for two cued locations, but dropped much more, to 74% correct or less for six cued locations.

It is difficult to know how these results can be translated into MOT tasks. The selection demands in a typical MOT task may be less taxing than in the @franconeriHowManyLocations2007 experiments, because participants need only maintain their attention on the objects, not search through them. However, it remains unclear how much less demanding that is, so we still do not know how much of the target-load effect in typical MOT displays can be attributed to failures of selection that would occur even were the objects to remain stationary.

In practice, people make many more errors in a tracking task if the targets move than if they do not move (CITATION NEEDED). This observation is not enough, however, to conclude that the limits on spatial selection are not the cause of most errors in moving-object tracking. One reason is that visual working memory can greatly benefit performance with static locations, but memory for locations likely does not update very well in the presence of motion, as will be discussed in section \@ref(identity).
-->

<!-- In some experiments, the targets are initially stationary, but nevertheless typically are easily selected as they flicker or are shown in a different color to make them highly salient [@drewNeuralMeasuresIndividual2008; @franconeriHowManyLocations2007].  -->

