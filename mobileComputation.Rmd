# Mobile computation 

We live in a world of constantly moving objects. The receptive fields in our early visual cortices that analyze the properties of objects are largely stationary. In the case of moving objects, these individual analyzers get only a glimpse of moving objects before they move on. These glimpses will sometimes be too brief to fully process the object. What makes things even worse is that in our world of moving objects, another object may quickly move onto the retinal location previously occupied by the old object, such that extended analysis by our stationary receptive fields may combine the two objects, yielding a mish-mash of features.

This phenomenon is demonstrated by the below movie (if it doesn't play, you should be able to view it [here](https://media.giphy.com/media/g4FWsHsyYwnphuEyh5/source.gif)). Keep your gaze fixed on the circle in the center, and try to judge for one of the flip-flopping objects whether the light green color is paired with leftward tilt or rightward tilt.
You may find this difficult or impossible to judge. 

```{r, echo=FALSE, out.width="100%", fig.cap="Task: fixate the circle and judge whether the red color is paired with leftward tilt or rightward tilt."}
#Work-around to make GIFs work by avoiding including them in non-html outputs!
#https://stackoverflow.com/questions/64038037/can-i-conditionally-exclude-some-elements-code-blocks-from-rendering-to-the-pd
if(knitr::is_html_output()) knitr::include_url("movies/OC4HzNoGuide.gif") else knitr::include_graphics("movies/OC4HzNoGuide_static.gif")
#, height = "250px"
```

Rarely in the real world do objects rapidly alternate as they do in the above movie. Instead, the reason that individual receptive fields sometimes get brief glimpses is because objects are rapidly moving rather than alternating. The brain capitalizes on this by accumulating the results of visual analysis from the successive locations that a moving object visits [@nishidaMotionbasedAnalysisSpatial2004a; @nishidaHumanVisualSystem2007a].

You may be able to experience this "mobile computation" in action by viewing the below movie (if it doesn't play below, you should be able to view it [here](movies/OC4HzGuide_static.gif)). This time, while keeping your gaze fixed on the circle in the center, try to track the white circle with your attention as it steps about the circle. This may allow you to judge whether the light green is paired with leftward or rightward tilt [@cavanaghMobileComputationSpatiotemporal2008].

```{r, echo=FALSE, out.width="100%", fig.cap="Task: fixate the circle and judge whether the light green is paired with leftward tilt or rightward title."}
#Work-around to make GIFs work by avoiding including them in non-html outputs!
#https://stackoverflow.com/questions/64038037/can-i-conditionally-exclude-some-elements-code-blocks-from-rendering-to-the-pd
if(knitr::is_html_output()) knitr::include_url("movies/OC4HzGuide.gif") else knitr::include_graphics("movies/OC4HzGuide_static.gif")
#, height = "250px"
``` 

This second movie is identical to the first except for the addition of the stepping white ring. Tracking the ring with attention feels almost automatic thanks to it being the only moving object. The continuous selection enabled by tracking here evidently provides later stages of visual processing with an extended view of the object, allowing it to deliver the feature pairing [@cavanaghMobileComputationSpatiotemporal2008]. This interpretation is consistent with ideas about feature pairing introduced by Treisman. The "spotlight" of selection gates later processing by later stages, which is what is required for feature pairing.

As we saw in a previous section (\@ref{bottlenecks}), tracking is capacity-limited; in many circumstances one can only track a few objects. Here we highlighted that tracking, like other instances of attentional selection, serves to gate what information is used by subsequent processing. So, what do we know about the limits of subsequent processing, and how those interact with attentional tracking?

## Attentional tracking and mobile computation

@holcombePerceivingSpatialRelations2011 investigated the relationship of attentional tracking to various judgments that can be made about two arrays of concentric colored circles. 
```{r, echo=FALSE, out.width="100%", fig.cap=""}
#Work-around to make GIFs (but not .mov) work by avoiding including them in non-html outputs!
#https://stackoverflow.com/questions/64038037/can-i-conditionally-exclude-some-elements-code-blocks-from-rendering-to-the-pd
if(knitr::is_html_output()) knitr::include_url("movies/LinaresVaziriPashkamHolcombe/pairingOneStimulusCycleSoRotatesPerenially.gif") else knitr::include_graphics("movies/LinaresVaziriPashkamHolcombe/pairingOneStimulusCycleSoRotatesPereniallyStatic.gif")
#, height = "250px"
```

With these arrays, participants can be asked to judge the individual colors that are present and also their spatial arrangement. At the speed of rotation seen above, it is quite easy for many to covertly track a disc while fixating their eyes on the array's center. At high speeds over 1.4 revolutions per second or so, however, @holcombePerceivingSpatialRelations2011 found that their participants could only guess at which object was originally cued at the beginning of the trial (tracking speed limits are discussed further in section BLAH). 

```{r, echo=FALSE, out.width="100%", fig.cap=""}
#Work-around to make GIFs (but not .mov) work by avoiding including them in non-html outputs!
#https://stackoverflow.com/questions/64038037/can-i-conditionally-exclude-some-elements-code-blocks-from-rendering-to-the-pd
if(knitr::is_html_output()) knitr::include_url("movies/LinaresVaziriPashkamHolcombe/pairing_3timesFasterOneStimulusCycleSoRotatesPerenially.gif") else knitr::include_graphics("movies/LinaresVaziriPashkamHolcombe/pairingOneStimulusCycleSoRotatesPereniallyStatic.gif")
#, height = "250px"
```

Above their tracking limit (a bit faster for most than the movie above), participants were still able to judge which colors were present in the display, indicating that tracking was not needed for that. <!--This was revealed by using different colors for the array on different trials and asking participants to report which colors were present.--> Participants were unable, however, to judge the spatial arrangement of the colors.

In one of the spatial arrangement tasks, the participants were asked to report any three colors in sequence from around a ring, in the direction of motion. For the display above, then, for the inner ring correct answers included "blue, yellow, purple" or "yellow, purple, blue", but not "yellow, blue, purple" (the participants made their response by clicking on colors with a mouse, so they did not need to know color names). In a second spatial arrangement task, participants were tasked with reporting any two colors that were aligned with each other between the two rings. The correct answers for the above display, then, were "green and yellow", "red and purple", and "blue and aqua".

For those two spatial arrangement tasks, the speed limit, as quantified by 75% threshold, was similar to (and statistically indistinguishable from) the tracking limit. This suggests that tracking is required to apprehend the spatial arrangement. Identifying the colors, in contrast, could be done at much faster speeds, suggesting that tracking was not necessary for it.

A need for focused attention to apprehend spatial arrangements had already been strongly suggested by evidence from visual search and dual task paradigms [@liRapidNaturalScene2002; @leeAttentionalCapacityUndifferentiated1999a; @loganSpatialAttentionApprehension1994; @wolfeWhatCan0001998]. Dual task paradigms had also already found that the colors presented could be identified even in the presence of very demanding secondary tasks that greatly reduce the amount of attention available [@leeAttentionalCapacityUndifferentiated1999a; @braunWithdrawingAttentionLittle1998a]. Here, then, our tracking experiments converged with those results, indicating that the same kind of attentional resource whose allocation to a target is reduced by increasing the set size in visual search and by a demanding secondary task is also eliminated by moving objects faster than they can be tracked.

Not all spatial relationships, however, require an ample amount of focused attention. In the @holcombePerceivingSpatialRelations2011 study, one particular spatial relationship, alignment of the inner and outer rings, could be judged even at object speeds far above the tracking speed limit. For that alignment task, the two concentric rings comprised the same colors in the same sequence. On some trials, these colors were aligned, so that neighboring colors in the radial direction were identical. Participants had to discriminate between that and when neighboring colors in the radial direction were different. While the participants' mean tracking speed threshold was 1.4 rps, their speed threshold for the alignment task was 2.4 rps. The processes that mediate the alignment judgment are not clear, but likely are those pre-attentive mechanisms that integrate information over space and time regarding spatial differences, for example to extract texture boundaries and global shape [@motoyoshiTemporalResolutionOrientationbased2001; @ramachandranPhantomContoursNew1991a].

Such boundary and global shape processing seem to integrate across multiple features, but do not pass on information about the constituent features (here, colors) whose arrangement created the boundary or shape [@wilsonDetectionGlobalStructure1998; @cliffordRapidGlobalForm2004]. It seems that only focused attention can do that. But how does focused attention do it? One idea that probably goes way back is that to apprehend a spatial relationship, attention must identify the colors in the colors' locations one by one e.g., @huangCharacterizingLimitsHuman2007].

A particular pattern of errors made by participants in the experiments of @holcombePerceivingSpatialRelations2011 supports the serial processing theory. Recall the spatial arrangement task in which participants were required to report two of the colors aligned with each other in the inner and outer rings. In a variation of that task, each trial began with a central color cue. That color could be in either the inner ring or the outer ring. Participants were told to report which color in the other ring was aligned with that color. The speed of the rings were set to slightly below the tracking speed limit. Thanks to feature-based attention, finding the cued color was not difficult [@shihThereFeaturebasedAttentional1996]. Often the participants reported correctly the color that was aligned with the cued color. On those trials where participants made an error, they typically reported a color that was adjacent to the correct one, located either leading it (relative to the directon of motion) or trailing it. Critically, trailing errors were much more likely than leading errors. The best explanation of this, after a few control conditions were examined, appears to be that participants made a shift of attention from the cued ring to the other ring, but because the ring kept moving during that shift, they sometimes missed and landed on the trailing disc, resulting in a mistaken report of its color.

Using a completely different approach based on EEG and eye movements, @franconeriFlexibleVisualProcessing2011 also concluded that spatial relationships are calculated by identifying the constituent objects one-by-one. In summary, when objects move, attentional tracking is necessary to set up the focused attentional shifts that are needed to extract most spatial relationships.



 
